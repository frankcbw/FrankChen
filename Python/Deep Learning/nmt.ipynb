{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nmt.ipynb","provenance":[{"file_id":"1rHYoCXb96INsxCSc1G4OmZhisnuabsIH","timestamp":1584131022870},{"file_id":"1DDMy9pV02zKJRGgZWlUe3jZ3fas8lkcM","timestamp":1583785472008}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python2","display_name":"Python 2"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TjPTaRB4mpCd","colab_type":"text"},"source":["# Colab FAQ\n","\n","For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n","\n","You need to use the colab GPU for this assignmentby selecting:\n","\n","> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"]},{"cell_type":"markdown","metadata":{"id":"s9IS9B9-yUU5","colab_type":"text"},"source":["## Setup PyTorch\n","All files are stored at /content/csc421/a3/ folder\n"]},{"cell_type":"markdown","metadata":{"id":"axbuunY8UdTB","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"Z-6MQhMOlHXD","colab_type":"code","outputId":"7493d2d5-9ad8-4dbc-dce7-340a5cffcc60","executionInfo":{"status":"ok","timestamp":1584573274683,"user_tz":240,"elapsed":9678,"user":{"displayName":"Frank Chen","photoUrl":"","userId":"12545694808850748741"}},"colab":{"base_uri":"https://localhost:8080/","height":446}},"source":["######################################################################\n","# Setup python environment and change the current working directory\n","######################################################################\n","!pip install torch torchvision\n","!pip install Pillow==4.0.0\n","%mkdir -p /content/csc421/a3/\n","%cd /content/csc421/a3"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python2.7/dist-packages (1.4.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python2.7/dist-packages (0.5.0)\n","Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.16.4)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python2.7/dist-packages (from torchvision) (4.3.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python2.7/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n","Collecting Pillow==4.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/99/0e3522a9764fe371bf9f7729404b1ef7d9c4fc49cbe5f1761c6e07812345/Pillow-4.0.0-cp27-cp27mu-manylinux1_x86_64.whl (5.6MB)\n","\u001b[K     |████████████████████████████████| 5.6MB 13.9MB/s \n","\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python2.7/dist-packages (from Pillow==4.0.0) (0.46)\n","\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: scikit-image 0.14.3 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: torchvision 0.5.0 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n","Installing collected packages: Pillow\n","  Found existing installation: Pillow 4.3.0\n","    Uninstalling Pillow-4.3.0:\n","      Successfully uninstalled Pillow-4.3.0\n","Successfully installed Pillow-4.0.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["/content/csc421/a3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9DaTdRNuUra7","colab_type":"text"},"source":["# Helper code"]},{"cell_type":"markdown","metadata":{"id":"4BIpGwANoQOg","colab_type":"text"},"source":["## Utility functions"]},{"cell_type":"code","metadata":{"id":"D-UJHBYZkh7f","colab_type":"code","colab":{}},"source":["import os\n","import pdb\n","import argparse\n","import pickle as pkl\n","\n","from collections import defaultdict\n","\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","from six.moves.urllib.request import urlretrieve\n","import tarfile\n","import pickle\n","import sys\n","\n","\n","def get_file(fname,\n","             origin,\n","             untar=False,\n","             extract=False,\n","             archive_format='auto',\n","             cache_dir='data'):\n","    datadir = os.path.join(cache_dir)\n","    if not os.path.exists(datadir):\n","        os.makedirs(datadir)\n","\n","    if untar:\n","        untar_fpath = os.path.join(datadir, fname)\n","        fpath = untar_fpath + '.tar.gz'\n","    else:\n","        fpath = os.path.join(datadir, fname)\n","    \n","    print(fpath)\n","    if not os.path.exists(fpath):\n","        print('Downloading data from', origin)\n","\n","        error_msg = 'URL fetch failure on {}: {} -- {}'\n","        try:\n","            try:\n","                urlretrieve(origin, fpath)\n","            except URLError as e:\n","                raise Exception(error_msg.format(origin, e.errno, e.reason))\n","            except HTTPError as e:\n","                raise Exception(error_msg.format(origin, e.code, e.msg))\n","        except (Exception, KeyboardInterrupt) as e:\n","            if os.path.exists(fpath):\n","                os.remove(fpath)\n","            raise\n","\n","    if untar:\n","        if not os.path.exists(untar_fpath):\n","            print('Extracting file.')\n","            with tarfile.open(fpath) as archive:\n","                archive.extractall(datadir)\n","        return untar_fpath\n","\n","    if extract:\n","        _extract_archive(fpath, datadir, archive_format)\n","\n","    return fpath\n","\n","class AttrDict(dict):\n","    def __init__(self, *args, **kwargs):\n","        super(AttrDict, self).__init__(*args, **kwargs)\n","        self.__dict__ = self\n","        \n","def to_var(tensor, cuda):\n","    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n","\n","        Arguments:\n","            tensor: A Tensor object.\n","            cuda: A boolean flag indicating whether to use the GPU.\n","\n","        Returns:\n","            A Variable object, on the GPU if cuda==True.\n","    \"\"\"\n","    if cuda:\n","        return Variable(tensor.cuda())\n","    else:\n","        return Variable(tensor)\n","\n","\n","def create_dir_if_not_exists(directory):\n","    \"\"\"Creates a directory if it doesn't already exist.\n","    \"\"\"\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","\n","def save_loss_plot(train_losses, val_losses, opts):\n","    \"\"\"Saves a plot of the training and validation loss curves.\n","    \"\"\"\n","    plt.figure()\n","    plt.plot(range(len(train_losses)), train_losses)\n","    plt.plot(range(len(val_losses)), val_losses)\n","    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n","    plt.xlabel('Epochs', fontsize=16)\n","    plt.ylabel('Loss', fontsize=16)\n","    plt.xticks(fontsize=14)\n","    plt.yticks(fontsize=14)\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n","    plt.close()\n","\n","\n","def checkpoint(encoder, decoder, idx_dict, opts):\n","    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n","    contains the char_to_index and index_to_char mappings, and the start_token\n","    and end_token values.\n","    \"\"\"\n","    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n","        torch.save(encoder, f)\n","\n","    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n","        torch.save(decoder, f)\n","\n","    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n","        pkl.dump(idx_dict, f)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pbvpn4MaV0I1","colab_type":"text"},"source":["## Data loader"]},{"cell_type":"code","metadata":{"id":"XVT4TNTOV3Eg","colab_type":"code","colab":{}},"source":["def read_lines(filename):\n","    \"\"\"Read a file and split it into lines.\n","    \"\"\"\n","    lines = open(filename).read().strip().lower().split('\\n')\n","    return lines\n","\n","\n","def read_pairs(filename):\n","    \"\"\"Reads lines that consist of two words, separated by a space.\n","\n","    Returns:\n","        source_words: A list of the first word in each line of the file.\n","        target_words: A list of the second word in each line of the file.\n","    \"\"\"\n","    lines = read_lines(filename)\n","    source_words, target_words = [], []\n","    for line in lines:\n","        line = line.strip()\n","        if line:\n","            source, target = line.split()\n","            source_words.append(source)\n","            target_words.append(target)\n","    return source_words, target_words\n","\n","\n","def all_alpha_or_dash(s):\n","    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n","    \"\"\"\n","    return all(c.isalpha() or c == '-' for c in s)\n","\n","\n","def filter_lines(lines):\n","    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n","    \"\"\"\n","    return [line for line in lines if all_alpha_or_dash(line)]\n","\n","\n","def load_data():\n","    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n","    \"\"\"\n","\n","    source_lines, target_lines = read_pairs('data/pig_latin_data.txt')\n","\n","    # Filter lines\n","    source_lines = filter_lines(source_lines)\n","    target_lines = filter_lines(target_lines)\n","\n","    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n","\n","    # Create a dictionary mapping each character to a unique index\n","    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n","\n","    # Add start and end tokens to the dictionary\n","    start_token = len(char_to_index)\n","    end_token = len(char_to_index) + 1\n","    char_to_index['SOS'] = start_token\n","    char_to_index['EOS'] = end_token\n","\n","    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n","    index_to_char = { index: char for (char, index) in char_to_index.items() }\n","\n","    # Store the final size of the vocabulary\n","    vocab_size = len(char_to_index)\n","\n","    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n","\n","    idx_dict = { 'char_to_index': char_to_index,\n","                 'index_to_char': index_to_char,\n","                 'start_token': start_token,\n","                 'end_token': end_token }\n","\n","    return line_pairs, vocab_size, idx_dict\n","\n","\n","def create_dict(pairs):\n","    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n","    This is used to make batches: each batch consists of two parallel tensors, one containing\n","    all source indexes and the other containing all corresponding target indexes.\n","    Within a batch, all the source words are the same length, and all the target words are\n","    the same length.\n","    \"\"\"\n","    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n","\n","    d = defaultdict(list)\n","    for (s,t) in unique_pairs:\n","        d[(len(s), len(t))].append((s,t))\n","\n","    return d\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bRWfRdmVVjUl","colab_type":"text"},"source":["## Training and evaluation code"]},{"cell_type":"code","metadata":{"id":"wa5-onJhoSeM","colab_type":"code","colab":{}},"source":["def string_to_index_list(s, char_to_index, end_token):\n","    \"\"\"Converts a sentence into a list of indexes (for each character).\n","    \"\"\"\n","    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n","\n","\n","def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n","    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n","    words (whitespace-separated), running the encoder-decoder model to translate each\n","    word independently, and then stitching the words back together with spaces between them.\n","    \"\"\"\n","    if idx_dict is None:\n","      line_pairs, vocab_size, idx_dict = load_data()\n","    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n","\n","\n","def translate(input_string, encoder, decoder, idx_dict, opts):\n","    \"\"\"Translates a given string from English to Pig-Latin.\n","    \"\"\"\n","\n","    char_to_index = idx_dict['char_to_index']\n","    index_to_char = idx_dict['index_to_char']\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","\n","    max_generated_chars = 20\n","    gen_string = ''\n","\n","    indexes = string_to_index_list(input_string, char_to_index, end_token)\n","    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n","\n","    encoder_annotations, encoder_last_hidden = encoder(indexes)\n","\n","    decoder_hidden = encoder_last_hidden\n","    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n","    decoder_inputs = decoder_input\n","\n","    for i in range(max_generated_chars):\n","      ## slow decoding, recompute everything at each time\n","      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n","      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n","      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n","      ni = ni[-1] #latest output token\n","\n","      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n","      \n","      if ni == end_token:\n","          break\n","      else:\n","          gen_string = \"\".join(\n","              [index_to_char[int(item)] \n","               for item in generated_words.cpu().numpy().reshape(-1)])\n","\n","    return gen_string\n","\n","\n","def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n","    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n","    \"\"\"\n","    if idx_dict is None:\n","      line_pairs, vocab_size, idx_dict = load_data()\n","    char_to_index = idx_dict['char_to_index']\n","    index_to_char = idx_dict['index_to_char']\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","\n","    max_generated_chars = 20\n","    gen_string = ''\n","\n","    indexes = string_to_index_list(input_string, char_to_index, end_token)\n","    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n","\n","    encoder_annotations, encoder_hidden = encoder(indexes)\n","\n","    decoder_hidden = encoder_hidden\n","    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n","    decoder_inputs = decoder_input\n","\n","    produced_end_token = False\n","\n","    for i in range(max_generated_chars):\n","      ## slow decoding, recompute everything at each time\n","      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n","      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n","      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n","      ni = ni[-1] #latest output token\n","      \n","      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n","      \n","      if ni == end_token:\n","          break\n","      else:\n","          gen_string = \"\".join(\n","              [index_to_char[int(item)] \n","               for item in generated_words.cpu().numpy().reshape(-1)])\n","    \n","    if isinstance(attention_weights, tuple):\n","      ## transformer's attention mweights\n","      attention_weights, self_attention_weights = attention_weights\n","    \n","    all_attention_weights = attention_weights.data.cpu().numpy()\n","    \n","    for i in range(len(all_attention_weights)):\n","      attention_weights_matrix = all_attention_weights[i].squeeze()\n","      fig = plt.figure()\n","      ax = fig.add_subplot(111)\n","      cax = ax.matshow(attention_weights_matrix, cmap='bone')\n","      fig.colorbar(cax)\n","\n","      # Set up axes\n","      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n","      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n","\n","      # Show label at every tick\n","      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","      # Add title\n","      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n","      plt.tight_layout()\n","      plt.grid('off')\n","      plt.show()\n","      plt.savefig(save)\n","\n","      plt.close(fig)\n","\n","    return gen_string\n","\n","\n","def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n","    \"\"\"Train/Evaluate the model on a dataset.\n","\n","    Arguments:\n","        data_dict: The validation/test word pairs, organized by source and target lengths.\n","        encoder: An encoder model to produce annotations for each step of the input sequence.\n","        decoder: A decoder model (with or without attention) to generate output tokens.\n","        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n","        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n","        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n","        opts: The command-line arguments.\n","\n","    Returns:\n","        mean_loss: The average loss over all batches from data_dict.\n","    \"\"\"\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","    char_to_index = idx_dict['char_to_index']\n","\n","    losses = []\n","    for key in data_dict:\n","        input_strings, target_strings = zip(*data_dict[key])\n","        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n","        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n","\n","        num_tensors = len(input_tensors)\n","        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n","\n","        for i in range(num_batches):\n","\n","            start = i * opts.batch_size\n","            end = start + opts.batch_size\n","\n","            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n","            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n","\n","            # The batch size may be different in each epoch\n","            BS = inputs.size(0)\n","\n","            encoder_annotations, encoder_hidden = encoder(inputs)\n","\n","            # The last hidden state of the encoder becomes the first hidden state of the decoder\n","            decoder_hidden = encoder_hidden\n","\n","            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n","            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n","\n","            loss = 0.0\n","\n","            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n","\n","            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n","            \n","            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n","            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n","            targets_flatten = targets.view(-1)\n","            loss = criterion(decoder_outputs_flatten, targets_flatten)\n","\n","            losses.append(loss.item())\n","\n","            ## training if an optimizer is provided\n","            if optimizer:\n","              # Zero gradients\n","              optimizer.zero_grad()\n","              # Compute gradients\n","              loss.backward()\n","              # Update the parameters of the encoder and decoder\n","              optimizer.step()\n","              \n","    mean_loss = np.mean(losses)\n","    return mean_loss\n","\n","  \n","\n","def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n","    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n","        * Prints training and val loss each epoch.\n","        * Prints qualitative translation results each epoch using TEST_SENTENCE\n","        * Saves an attention map for TEST_WORD_ATTN each epoch\n","\n","    Arguments:\n","        train_dict: The training word pairs, organized by source and target lengths.\n","        val_dict: The validation word pairs, organized by source and target lengths.\n","        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n","        encoder: An encoder model to produce annotations for each step of the input sequence.\n","        decoder: A decoder model (with or without attention) to generate output tokens.\n","        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n","        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n","        opts: The command-line arguments.\n","    \"\"\"\n","\n","    start_token = idx_dict['start_token']\n","    end_token = idx_dict['end_token']\n","    char_to_index = idx_dict['char_to_index']\n","\n","    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n","\n","    best_val_loss = 1e6\n","    train_losses = []\n","    val_losses = []\n","\n","    for epoch in range(opts.nepochs):\n","\n","        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n","        \n","        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n","        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n","\n","        if val_loss < best_val_loss:\n","            checkpoint(encoder, decoder, idx_dict, opts)\n","\n","        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n","        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n","\n","        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n","        loss_log.flush()\n","\n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","\n","        save_loss_plot(train_losses, val_losses, opts)\n","\n","\n","def print_data_stats(line_pairs, vocab_size, idx_dict):\n","    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n","    \"\"\"\n","    print('=' * 80)\n","    print('Data Stats'.center(80))\n","    print('-' * 80)\n","    for pair in line_pairs[:5]:\n","        print(pair)\n","    print('Num unique word pairs: {}'.format(len(line_pairs)))\n","    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n","    print('Vocab size: {}'.format(vocab_size))\n","    print('=' * 80)\n","\n","\n","def train(opts):\n","    line_pairs, vocab_size, idx_dict = load_data()\n","    print_data_stats(line_pairs, vocab_size, idx_dict)\n","\n","    # Split the line pairs into an 80% train and 20% val split\n","    num_lines = len(line_pairs)\n","    num_train = int(0.8 * num_lines)\n","    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n","\n","    # Group the data by the lengths of the source and target words, to form batches\n","    train_dict = create_dict(train_pairs)\n","    val_dict = create_dict(val_pairs)\n","\n","    ##########################################################################\n","    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n","    ##########################################################################\n","    if opts.encoder_type == \"rnn\":\n","      encoder = GRUEncoder(vocab_size=vocab_size, \n","                          hidden_size=opts.hidden_size, \n","                          opts=opts)\n","    elif opts.encoder_type == \"transformer\":\n","      encoder = TransformerEncoder(vocab_size=vocab_size, \n","                                   hidden_size=opts.hidden_size, \n","                                   num_layers=opts.num_transformer_layers,\n","                                   opts=opts)\n","    else:\n","        raise NotImplementedError\n","\n","    if opts.decoder_type == 'rnn':\n","        decoder = RNNDecoder(vocab_size=vocab_size, \n","                             hidden_size=opts.hidden_size)\n","    elif opts.decoder_type == 'rnn_attention':\n","        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n","                                      hidden_size=opts.hidden_size, \n","                                      attention_type=opts.attention_type)\n","    elif opts.decoder_type == 'transformer':\n","        decoder = TransformerDecoder(vocab_size=vocab_size, \n","                                     hidden_size=opts.hidden_size, \n","                                     num_layers=opts.num_transformer_layers)\n","    else:\n","        raise NotImplementedError\n","        \n","    #### setup checkpoint path\n","    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n","                                      opts.batch_size, \n","                                      opts.decoder_type)\n","    opts.checkpoint_path = model_name\n","    create_dir_if_not_exists(opts.checkpoint_path)\n","    ####\n","\n","    if opts.cuda:\n","        encoder.cuda()\n","        decoder.cuda()\n","        print(\"Moved models to GPU!\")\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n","\n","    try:\n","        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n","    except KeyboardInterrupt:\n","        print('Exiting early from training.')\n","        return encoder, decoder\n","      \n","    return encoder, decoder\n","\n","\n","def print_opts(opts):\n","    \"\"\"Prints the values of all command-line arguments.\n","    \"\"\"\n","    print('=' * 80)\n","    print('Opts'.center(80))\n","    print('-' * 80)\n","    for key in opts.__dict__:\n","        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n","    print('=' * 80)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0yh08KhgnA30","colab_type":"text"},"source":["## Download dataset"]},{"cell_type":"code","metadata":{"id":"aROU2xZanDKq","colab_type":"code","outputId":"f8c50aaf-2942-4a57-c0db-02245a922c09","executionInfo":{"status":"ok","timestamp":1584573278600,"user_tz":240,"elapsed":13521,"user":{"displayName":"Frank Chen","photoUrl":"","userId":"12545694808850748741"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["######################################################################\n","# Download Translation datasets\n","######################################################################\n","data_fpath = get_file(fname='pig_latin_data.txt', \n","                         origin='http://www.cs.toronto.edu/~jba/pig_latin_data.txt', \n","                         untar=False)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["data/pig_latin_data.txt\n","('Downloading data from', 'http://www.cs.toronto.edu/~jba/pig_latin_data.txt')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YDYMr7NclZdw","colab_type":"text"},"source":["# Part 1: Gated Recurrent Unit (GRU"]},{"cell_type":"markdown","metadata":{"id":"dCae1mOUlZrC","colab_type":"text"},"source":["## Step 1: GRU Cell\n","Please implement the Gated Recurent Unit class defined in the next cell. "]},{"cell_type":"code","metadata":{"id":"3HMO7FD6l5RU","colab_type":"code","colab":{}},"source":["class MyGRUCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(MyGRUCell, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        ## Input linear layers\n","        self.Wiz = nn.Linear(input_size, hidden_size)\n","        self.Wir = nn.Linear(input_size, hidden_size)\n","        self.Wih = nn.Linear(input_size, hidden_size)\n","\n","        ## Hidden linear layers\n","        self.Whz = nn.Linear(hidden_size, hidden_size)\n","        self.Whr = nn.Linear(hidden_size, hidden_size)\n","        self.Whh = nn.Linear(hidden_size, hidden_size)\n","        \n","\n","    def forward(self, x, h_prev):\n","        \"\"\"Forward pass of the GRU computation for one time step.\n","\n","        Arguments\n","            x: batch_size x input_size\n","            h_prev: batch_size x hidden_size\n","\n","        Returns:\n","            h_new: batch_size x hidden_size\n","        \"\"\"\n","\n","        # ------------\n","        # FILL THIS IN\n","        # ------------\n","        z = torch.sigmoid(self.Wiz(x)+self.Whz(h_prev))\n","        r = torch.sigmoid(self.Wir(x)+self.Whr(h_prev))\n","        g = torch.tanh(self.Wih(x)+torch.mul(r, self.Whh(h_prev)))\n","        h_new = torch.mul((1 - z),g) + torch.mul(z,h_prev);\n","        return h_new\n","\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ecEq4TP2lZ4Z","colab_type":"text"},"source":["## Step 2: GRU Encoder\n","Please inspect the following recurrent encoder/decoder implementations. Make sure to run the cells before proceeding. "]},{"cell_type":"code","metadata":{"id":"8jDNim2fmVJV","colab_type":"code","colab":{}},"source":["class GRUEncoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, opts):\n","        super(GRUEncoder, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.opts = opts\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.gru = MyGRUCell(hidden_size, hidden_size)\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward pass of the encoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n","\n","        Returns:\n","            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","        hidden = self.init_hidden(batch_size)\n","\n","        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n","        annotations = []\n","\n","        for i in range(seq_len):\n","            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n","            hidden = self.gru(x, hidden)\n","            annotations.append(hidden)\n","\n","        annotations = torch.stack(annotations, dim=1)\n","        return annotations, hidden\n","\n","    def init_hidden(self, bs):\n","        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n","        of a batch of sequences.\n","\n","        Arguments:\n","            bs: The batch size for the initial hidden state.\n","\n","        Returns:\n","            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n","        \"\"\"\n","        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HvwizYM9ma4p","colab_type":"code","colab":{}},"source":["class RNNDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size):\n","        super(RNNDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        \"\"\"Forward pass of the non-attentional decoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch. (batch_size x seq_len)\n","            annotations: This is not used here. It just maintains consistency with the\n","                    interface used by the AttentionDecoder class.\n","            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n","\n","        Returns:\n","            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n","            None        \n","        \"\"\"        \n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n","\n","        hiddens = []\n","        h_prev = hidden_init\n","        for i in range(seq_len):\n","            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n","            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n","            hiddens.append(h_prev)\n","\n","        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n","        \n","        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n","        return output, None  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TSDTbsydlaGI","colab_type":"text"},"source":["## Step 3: Training and Analysis\n","Train the following language model comprised of recurrent encoder and decoders. "]},{"cell_type":"code","metadata":{"id":"H3YLrAjsmx_W","colab_type":"code","outputId":"e1ba6987-daa5-4598-f27b-92876d673f94","executionInfo":{"status":"ok","timestamp":1584573748242,"user_tz":240,"elapsed":483133,"user":{"displayName":"Frank Chen","photoUrl":"","userId":"12545694808850748741"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["TEST_SENTENCE = 'the air conditioning is working'\n","\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005, \n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'encoder_type': 'rnn', # options: rnn / transformer\n","              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n","              'attention_type': '',  # options: additive / scaled_dot\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","rnn_encoder, rnn_decoder = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                            hidden_size: 20                                     \n","                          learning_rate: 0.005                                  \n","                             batch_size: 64                                     \n","                                nepochs: 100                                    \n","                           encoder_type: rnn                                    \n","                                   cuda: 1                                      \n","                         checkpoint_dir: checkpoints                            \n","                           decoder_type: rnn                                    \n","                               lr_decay: 0.99                                   \n","                         attention_type:                                        \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('payment', 'aymentpay')\n","('ordination', 'ordinationway')\n","('amends', 'amendsway')\n","('principally', 'incipallypray')\n","('anybody', 'anybodyway')\n","Num unique word pairs: 6387\n","Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type GRUEncoder. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type MyGRUCell. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type RNNDecoder. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch:   0 | Train loss: 2.440 | Val loss: 2.024 | Gen: esay esay esay esay esay\n","Epoch:   1 | Train loss: 1.964 | Val loss: 1.874 | Gen: esay angesay onteday ingeday onteday\n","Epoch:   2 | Train loss: 1.812 | Val loss: 1.772 | Gen: eateday arteday ontedsay ingedsay onteday\n","Epoch:   3 | Train loss: 1.709 | Val loss: 1.713 | Gen: estay artedway ontedsay ingedway onsedsay\n","Epoch:   4 | Train loss: 1.641 | Val loss: 1.659 | Gen: eatedway artedway ontersay ingedway ontersay\n","Epoch:   5 | Train loss: 1.586 | Val loss: 1.635 | Gen: eatedway artedway onsersay ingsay onsersay\n","Epoch:   6 | Train loss: 1.544 | Val loss: 1.598 | Gen: elay artedway ontersay ingsay onsersay\n","Epoch:   7 | Train loss: 1.503 | Val loss: 1.571 | Gen: eatedway artedsay ontersay ingsay ontersay\n","Epoch:   8 | Train loss: 1.472 | Val loss: 1.547 | Gen: eatedway aringway onterestay ingway ontersay\n","Epoch:   9 | Train loss: 1.435 | Val loss: 1.506 | Gen: eatthay aringway onterstay ingway onterstay\n","Epoch:  10 | Train loss: 1.406 | Val loss: 1.494 | Gen: eatthay aringway onterstay ingway onterstay\n","Epoch:  11 | Train loss: 1.379 | Val loss: 1.477 | Gen: eatthay aingway onterstay ingway onderedsay\n","Epoch:  12 | Train loss: 1.347 | Val loss: 1.463 | Gen: eatthay aingway onsingsay ingway onderedsay\n","Epoch:  13 | Train loss: 1.327 | Val loss: 1.450 | Gen: eatthay aingway onsingsay inglay onderedsay\n","Epoch:  14 | Train loss: 1.297 | Val loss: 1.433 | Gen: earay aingway onsingsay inglay onterstay\n","Epoch:  15 | Train loss: 1.277 | Val loss: 1.428 | Gen: estay aingsay onsingsay inglay ondersedway\n","Epoch:  16 | Train loss: 1.252 | Val loss: 1.415 | Gen: earay aingway onsingeray inglay onderestay\n","Epoch:  17 | Train loss: 1.252 | Val loss: 1.406 | Gen: eathtay aintedway onsingsay inglay onderestay\n","Epoch:  18 | Train loss: 1.245 | Val loss: 1.383 | Gen: eatedway aingsay onsingsay inglay onderestay\n","Epoch:  19 | Train loss: 1.202 | Val loss: 1.370 | Gen: eatsway aingssay onsingersay inglay onderestay\n","Epoch:  20 | Train loss: 1.186 | Val loss: 1.356 | Gen: eatsway aightray onsingerssay inglay ondersestay\n","Epoch:  21 | Train loss: 1.192 | Val loss: 1.398 | Gen: eatway aringssay ontingssay ishtay ondersingway\n","Epoch:  22 | Train loss: 1.209 | Val loss: 1.342 | Gen: eatedway aightray ontingsay iway ontrestay\n","Epoch:  23 | Train loss: 1.176 | Val loss: 1.332 | Gen: eathsay aithay ontingsay ischay orringssay\n","Epoch:  24 | Train loss: 1.137 | Val loss: 1.318 | Gen: eatsway airday ontingsay iscay orringway\n","Epoch:  25 | Train loss: 1.118 | Val loss: 1.306 | Gen: eachedway airday ontingsay ischay orringssay\n","Epoch:  26 | Train loss: 1.107 | Val loss: 1.297 | Gen: eachedway airday ontingssay ischay orringssay\n","Epoch:  27 | Train loss: 1.101 | Val loss: 1.297 | Gen: eachedway airday ontingsay islay orringway\n","Epoch:  28 | Train loss: 1.107 | Val loss: 1.302 | Gen: eachedway airday ontingssay ischay ortingray\n","Epoch:  29 | Train loss: 1.082 | Val loss: 1.280 | Gen: eachedway airday ontingssay ischay orringway\n","Epoch:  30 | Train loss: 1.067 | Val loss: 1.280 | Gen: eachedway airday ontingssay ischay orringssay\n","Epoch:  31 | Train loss: 1.055 | Val loss: 1.273 | Gen: eachedway airday ontingssay ischay orringsray\n","Epoch:  32 | Train loss: 1.050 | Val loss: 1.265 | Gen: eachedway airway ontingssay islay ortingray\n","Epoch:  33 | Train loss: 1.078 | Val loss: 1.288 | Gen: eatsway airtray ontingssay islay ortingray\n","Epoch:  34 | Train loss: 1.072 | Val loss: 1.265 | Gen: eathay airway ontingssay isedway ortiredway\n","Epoch:  35 | Train loss: 1.048 | Val loss: 1.250 | Gen: eathay airway ontingstay islay ortiredway\n","Epoch:  36 | Train loss: 1.028 | Val loss: 1.250 | Gen: eathay airday ontiningway islay orringsray\n","Epoch:  37 | Train loss: 1.014 | Val loss: 1.246 | Gen: eathay airday ontingssay islay orringsray\n","Epoch:  38 | Train loss: 1.005 | Val loss: 1.241 | Gen: eathay airday ontingssay islay ortiredway\n","Epoch:  39 | Train loss: 0.999 | Val loss: 1.243 | Gen: eachedway airday ontingshay islay ortiredway\n","Epoch:  40 | Train loss: 0.999 | Val loss: 1.239 | Gen: eathay airway ontingssay islay ortingray\n","Epoch:  41 | Train loss: 0.998 | Val loss: 1.250 | Gen: eathay airway ontiningsay islay ortingway\n","Epoch:  42 | Train loss: 1.004 | Val loss: 1.220 | Gen: eachedway airway ontingshay isedway ortingray\n","Epoch:  43 | Train loss: 0.987 | Val loss: 1.239 | Gen: eshay airway ontingerhay issway ortingray\n","Epoch:  44 | Train loss: 0.979 | Val loss: 1.221 | Gen: eachedway airway ontingstay islay ortiray\n","Epoch:  45 | Train loss: 0.978 | Val loss: 1.213 | Gen: eachedway airday ontingerhay isedway ortingray\n","Epoch:  46 | Train loss: 0.965 | Val loss: 1.225 | Gen: eachedway airway ontingsthay isshay ortiray\n","Epoch:  47 | Train loss: 0.958 | Val loss: 1.211 | Gen: eshay airway ontiningway islay ortingray\n","Epoch:  48 | Train loss: 0.965 | Val loss: 1.215 | Gen: ehay airday ontiningray issway ortiredway\n","Epoch:  49 | Train loss: 0.966 | Val loss: 1.204 | Gen: eshay airway ontiningway islay ortingray\n","Epoch:  50 | Train loss: 0.942 | Val loss: 1.193 | Gen: eshay airway ontiningway issway ortingray\n","Epoch:  51 | Train loss: 0.940 | Val loss: 1.200 | Gen: eshay airyday ontiningsay islay ortingray\n","Epoch:  52 | Train loss: 0.930 | Val loss: 1.204 | Gen: eshay airday ontiningray issway ortingway\n","Epoch:  53 | Train loss: 0.930 | Val loss: 1.194 | Gen: eshay airyday ontininghay issway ortingray\n","Epoch:  54 | Train loss: 0.926 | Val loss: 1.197 | Gen: eshay airway ontiningray issway ortingway\n","Epoch:  55 | Train loss: 0.925 | Val loss: 1.191 | Gen: eshay airday ontininghay issway ortingway\n","Epoch:  56 | Train loss: 0.929 | Val loss: 1.188 | Gen: eshay airway ontiningray issway ortingway\n","Epoch:  57 | Train loss: 0.926 | Val loss: 1.188 | Gen: eshay airyday ontiningray issway ortingway\n","Epoch:  58 | Train loss: 0.946 | Val loss: 1.207 | Gen: eshay airyway onttunestay issway ortray\n","Epoch:  59 | Train loss: 0.938 | Val loss: 1.183 | Gen: ehayday airyday onttingstay issway ortingway\n","Epoch:  60 | Train loss: 0.913 | Val loss: 1.181 | Gen: eshay airyday ontiningray issway ortingray\n","Epoch:  61 | Train loss: 0.895 | Val loss: 1.168 | Gen: ehay airyday ontininghay issway ortingway\n","Epoch:  62 | Train loss: 0.891 | Val loss: 1.173 | Gen: eshay airyday ontininghay issway ortingway\n","Epoch:  63 | Train loss: 0.884 | Val loss: 1.165 | Gen: eshay airyday ontiningray issway ortingway\n","Epoch:  64 | Train loss: 0.884 | Val loss: 1.171 | Gen: eshay airyday ontininghay isshay ortingway\n","Epoch:  65 | Train loss: 0.883 | Val loss: 1.163 | Gen: eshay airyday ontininghay issway ortmray\n","Epoch:  66 | Train loss: 0.889 | Val loss: 1.170 | Gen: ehayday airyday ontiningray isshay ortmray\n","Epoch:  67 | Train loss: 0.886 | Val loss: 1.165 | Gen: eshay airyday ontinterhay issway ortmray\n","Epoch:  68 | Train loss: 0.868 | Val loss: 1.156 | Gen: eshay airyday ontinterhay-onsay issway ortmray\n","Epoch:  69 | Train loss: 0.865 | Val loss: 1.158 | Gen: eshay airyday ontinterhay-onway isshay ortmray\n","Epoch:  70 | Train loss: 0.862 | Val loss: 1.161 | Gen: eshay airyday ontinterhay issgway ortmray\n","Epoch:  71 | Train loss: 0.869 | Val loss: 1.157 | Gen: eshay airyday ontinterhay-onsay isshay ortmray\n","Epoch:  72 | Train loss: 0.871 | Val loss: 1.160 | Gen: eshay airyday ontinterhay-onsay issway ortmray\n","Epoch:  73 | Train loss: 0.866 | Val loss: 1.154 | Gen: eshay airyday ontinterhay-onway issway ortmray\n","Epoch:  74 | Train loss: 0.855 | Val loss: 1.162 | Gen: eshay airway ontinterhay-oowfay issgway ortmray\n","Epoch:  75 | Train loss: 0.855 | Val loss: 1.147 | Gen: eshay airyday ontinterhay-onway issway ortmray\n","Epoch:  76 | Train loss: 0.867 | Val loss: 1.153 | Gen: eshay airyday ontinterhay-oonway issway orkingway\n","Epoch:  77 | Train loss: 0.849 | Val loss: 1.142 | Gen: eshay airway ontinterhay-oonway issgway ortmray\n","Epoch:  78 | Train loss: 0.838 | Val loss: 1.145 | Gen: eshay airway ontinterhay-oonway issgway orkingway\n","Epoch:  79 | Train loss: 0.834 | Val loss: 1.135 | Gen: eshay airway ontinterhay-onway issgway ortmray\n","Epoch:  80 | Train loss: 0.839 | Val loss: 1.154 | Gen: eshay airway ontinterhay-oonway issgway ortmay\n","Epoch:  81 | Train loss: 0.834 | Val loss: 1.144 | Gen: eshay airyday ontinterhay-onway issway orkingway\n","Epoch:  82 | Train loss: 0.831 | Val loss: 1.142 | Gen: eshay airway ontinterhay-oonway issay ortmray\n","Epoch:  83 | Train loss: 0.836 | Val loss: 1.136 | Gen: eshay airyday ontinterhay-oonway issgay orkingway\n","Epoch:  84 | Train loss: 0.840 | Val loss: 1.141 | Gen: eshay airyday ontinterhay-oonway issway orkingway\n","Epoch:  85 | Train loss: 0.825 | Val loss: 1.138 | Gen: eshay airday ontinterhay-oonway issay orkingway\n","Epoch:  86 | Train loss: 0.816 | Val loss: 1.133 | Gen: eshay airway ontinterhay-oonway issay ortmray\n","Epoch:  87 | Train loss: 0.847 | Val loss: 1.177 | Gen: eshay airyday ontinterhay issgway orkingway\n","Epoch:  88 | Train loss: 0.839 | Val loss: 1.127 | Gen: eshay airyday ontinterhay-onway issay ortmay\n","Epoch:  89 | Train loss: 0.814 | Val loss: 1.138 | Gen: eshay airway ontinterhay-ounway issay ortmay\n","Epoch:  90 | Train loss: 0.807 | Val loss: 1.132 | Gen: eshay airway ontitionday issay ortmray\n","Epoch:  91 | Train loss: 0.801 | Val loss: 1.127 | Gen: eshay airway ontitionday issay orkingway\n","Epoch:  92 | Train loss: 0.798 | Val loss: 1.127 | Gen: eshay airyday ontinterhay-ounnay issay orkingway\n","Epoch:  93 | Train loss: 0.796 | Val loss: 1.126 | Gen: eshay airway ontitionday issay orkingway\n","Epoch:  94 | Train loss: 0.794 | Val loss: 1.127 | Gen: eshay airway ontinterhay-ounnay issay orksingway\n","Epoch:  95 | Train loss: 0.795 | Val loss: 1.121 | Gen: eshay airday ontinterhay-onway issay orkingway\n","Epoch:  96 | Train loss: 0.804 | Val loss: 1.147 | Gen: eshay airway ontinterhay-onsay issway orksingway\n","Epoch:  97 | Train loss: 0.813 | Val loss: 1.134 | Gen: eshay airway ontinterhay-oongway issay orkingway\n","Epoch:  98 | Train loss: 0.822 | Val loss: 1.130 | Gen: eshay airyday ontinterhay-oonway issay orkingway\n","Epoch:  99 | Train loss: 0.805 | Val loss: 1.126 | Gen: eshay airyday ontinterhay-ounnay issay orkingway\n","source:\t\tthe air conditioning is working \n","translated:\teshay airyday ontinterhay-ounnay issay orkingway\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cE4ijaCzneAt","colab_type":"text"},"source":["Try translating different sentences by changing the variable TEST_SENTENCE. Identify two distinct failure modes and briefly describe them."]},{"cell_type":"code","metadata":{"id":"WrNnz8W1nULf","colab_type":"code","outputId":"9256418e-04da-4fe1-ba26-cefefb151052","executionInfo":{"status":"ok","timestamp":1584573748356,"user_tz":240,"elapsed":483235,"user":{"displayName":"Frank Chen","photoUrl":"","userId":"12545694808850748741"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["TEST_SENTENCE = 'the air conditioning is working'\n","translated = translate_sentence(TEST_SENTENCE, rnn_encoder, rnn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["source:\t\tthe air conditioning is working \n","translated:\teshay airyday ontinterhay-ounnay issay orkingway\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RWwA6OGqlaTq","colab_type":"text"},"source":["# Part 2: Additive Attention"]},{"cell_type":"markdown","metadata":{"id":"AJSafHSAmu_w","colab_type":"text"},"source":["## Step 1: Additive Attention\n","Already implemented the additive attention mechanism. Write down the mathematical expression for $\\tilde{\\alpha}_i^{(t)}, \\alpha_i^{(t)}, c_t$ as a function of $W_1, W_2, b_1, b_2, Q_t, K_i$. "]},{"cell_type":"code","metadata":{"id":"AdewEVSMo5jJ","colab_type":"code","colab":{}},"source":["class AdditiveAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(AdditiveAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        # A two layer fully-connected network\n","        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n","        self.attention_network = nn.Sequential(\n","                                    nn.Linear(hidden_size*2, hidden_size),\n","                                    nn.ReLU(),\n","                                    nn.Linear(hidden_size, 1)\n","                                 )\n","\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the additive attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state. (batch_size x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x 1 x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","\n","            The attention_weights must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","        batch_size = keys.size(0)\n","        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(keys)\n","        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n","        unnormalized_attention = self.attention_network(concat_inputs)\n","        attention_weights = self.softmax(unnormalized_attention)\n","        context = torch.bmm(attention_weights.transpose(2,1), values)\n","        return context, attention_weights\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"73_p8d5EmvOJ","colab_type":"text"},"source":["## Step 2: RNN Additive Attention Decoder\n","We will now implement a recurrent decoder that makes use of the additive attention mechanism. Read the description in the assignment worksheet and complete the following implementation. "]},{"cell_type":"code","metadata":{"id":"RJaABkXrpJSw","colab_type":"code","colab":{}},"source":["class RNNAttentionDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n","        super(RNNAttentionDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","\n","        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n","        if attention_type == 'additive':\n","          self.attention = AdditiveAttention(hidden_size=hidden_size)\n","        elif attention_type == 'scaled_dot':\n","          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n","        \n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","        \n","    def forward(self, inputs, annotations, hidden_init):\n","\n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n","\n","        hiddens = []\n","        attentions = []\n","        h_prev = hidden_init\n","        for i in range(seq_len):\n","          \n","            # Get the current time step, across the whole batch\n","            embed_current = embed[:,i,:]\n","            # batch_size x 1 x hidden_size  \n","            context, attention_weights = self.attention(h_prev, annotations,\n","                                                        annotations)\n","            # batch_size x (2*hidden_size) \n","            embed_and_context = torch.cat([context.reshape_as(embed_current),\n","                                           embed_current], dim=1)\n","            # batch_size x hidden_size    \n","            h_prev = self.rnn(embed_and_context, h_prev)     \n","  \n","            hiddens.append(h_prev)\n","            attentions.append(attention_weights)\n","        # batch_size x seq_len x hidden_size\n","        hiddens = torch.stack(hiddens, dim=1)\n","        # batch_size x seq_len x seq_len\n","        attentions = torch.cat(attentions, dim=2)\n","        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n","        return output, attentions\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vYPae08Io1Fi","colab_type":"text"},"source":["## Step 3: Training and Analysis\n","Train the following language model that uses a recurrent encoder, and a recurrent decoder that has an additive attention component. "]},{"cell_type":"code","metadata":{"id":"o3-FuzY1pepu","colab_type":"code","outputId":"d8de18f0-cdf0-4ad9-d1d4-ebb779492fc1","executionInfo":{"status":"ok","timestamp":1584574329398,"user_tz":240,"elapsed":1064261,"user":{"displayName":"Frank Chen","photoUrl":"","userId":"12545694808850748741"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["TEST_SENTENCE = 'the air conditioning is working'\n","\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005, \n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'encoder_type': 'rnn', # options: rnn / transformer\n","              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n","              'attention_type': 'additive',  # options: additive / scaled_dot\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","rnn_attn_encoder, rnn_attn_decoder = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                            hidden_size: 20                                     \n","                          learning_rate: 0.005                                  \n","                             batch_size: 64                                     \n","                                nepochs: 100                                    \n","                           encoder_type: rnn                                    \n","                                   cuda: 1                                      \n","                         checkpoint_dir: checkpoints                            \n","                           decoder_type: rnn_attention                          \n","                               lr_decay: 0.99                                   \n","                         attention_type: additive                               \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('payment', 'aymentpay')\n","('ordination', 'ordinationway')\n","('amends', 'amendsway')\n","('principally', 'incipallypray')\n","('anybody', 'anybodyway')\n","Num unique word pairs: 6387\n","Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type RNNAttentionDecoder. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type AdditiveAttention. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch:   0 | Train loss: 2.364 | Val loss: 1.963 | Gen: ereday areray annngesay ay enngeday\n","Epoch:   1 | Train loss: 1.864 | Val loss: 1.744 | Gen: ershay angray onngsay ay orighay\n","Epoch:   2 | Train loss: 1.626 | Val loss: 1.549 | Gen: eray aray oncontingay issay orighay\n","Epoch:   3 | Train loss: 1.417 | Val loss: 1.321 | Gen: ehthay arathay oncingsay issay ortinghay\n","Epoch:   4 | Train loss: 1.195 | Val loss: 1.125 | Gen: ehay agaray onstiongway issay orighay\n","Epoch:   5 | Train loss: 1.030 | Val loss: 1.027 | Gen: ehay ainhay ondionsingingway issay ortinghay\n","Epoch:   6 | Train loss: 0.814 | Val loss: 0.796 | Gen: ehay airay onditingway issay orkingway\n","Epoch:   7 | Train loss: 0.653 | Val loss: 0.677 | Gen: ehay airway onditiongway issay orkingway\n","Epoch:   8 | Train loss: 0.574 | Val loss: 0.684 | Gen: ehay airway onditiongay isway orkingway\n","Epoch:   9 | Train loss: 0.515 | Val loss: 0.673 | Gen: eway airway onditioncay isway orkingway\n","Epoch:  10 | Train loss: 0.483 | Val loss: 0.551 | Gen: eway airway onditioncay isway orfingway\n","Epoch:  11 | Train loss: 0.436 | Val loss: 0.544 | Gen: ehay airway onditioncay isway orkingway\n","Epoch:  12 | Train loss: 0.342 | Val loss: 0.429 | Gen: ehthay airway onditioncay isway orkingway\n","Epoch:  13 | Train loss: 0.289 | Val loss: 0.429 | Gen: ehay airway onditioncay isway orkingway\n","Epoch:  14 | Train loss: 0.253 | Val loss: 0.375 | Gen: ehay airway onditioningway isway orkingway\n","Epoch:  15 | Train loss: 0.275 | Val loss: 0.529 | Gen: ehay airway onditioningway isway orkingway\n","Epoch:  16 | Train loss: 0.292 | Val loss: 0.362 | Gen: ehay airway onditioningway isway orkingway\n","Epoch:  17 | Train loss: 0.203 | Val loss: 0.285 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  18 | Train loss: 0.240 | Val loss: 0.367 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  19 | Train loss: 0.213 | Val loss: 0.290 | Gen: ehay airway onditioningcay isway orkingway\n","Epoch:  20 | Train loss: 0.130 | Val loss: 0.246 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  21 | Train loss: 0.126 | Val loss: 0.230 | Gen: eway airway onditiongway isway orkingway\n","Epoch:  22 | Train loss: 0.129 | Val loss: 0.226 | Gen: eway airway onditiongcay isway orkingway\n","Epoch:  23 | Train loss: 0.097 | Val loss: 0.221 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  24 | Train loss: 0.084 | Val loss: 0.171 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  25 | Train loss: 0.080 | Val loss: 0.298 | Gen: eway airway onditiongway isway orkingway\n","Epoch:  26 | Train loss: 0.121 | Val loss: 0.222 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  27 | Train loss: 0.078 | Val loss: 0.178 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  28 | Train loss: 0.059 | Val loss: 0.159 | Gen: elay airway onditioningcay isway orkingway\n","Epoch:  29 | Train loss: 0.100 | Val loss: 0.222 | Gen: eray airway onditioncingcay isway orkingway\n","Epoch:  30 | Train loss: 0.149 | Val loss: 0.297 | Gen: elay airway ondioningcay isway orkingway\n","Epoch:  31 | Train loss: 0.094 | Val loss: 0.291 | Gen: ethay airway onditioncay isway orkingway\n","Epoch:  32 | Train loss: 0.072 | Val loss: 0.180 | Gen: elay airway onditioningcay isway orkingway\n","Epoch:  33 | Train loss: 0.055 | Val loss: 0.151 | Gen: elay airway onditioningcay isway orkingway\n","Epoch:  34 | Train loss: 0.051 | Val loss: 0.126 | Gen: elay airway onditioningcay isway orkingway\n","Epoch:  35 | Train loss: 0.154 | Val loss: 0.260 | Gen: elay airway onditioncay isway orkingway\n","Epoch:  36 | Train loss: 0.081 | Val loss: 0.135 | Gen: elay airway onditioningcay isway orkingway\n","Epoch:  37 | Train loss: 0.036 | Val loss: 0.114 | Gen: elay airway onditioningcay isway orkingway\n","Epoch:  38 | Train loss: 0.029 | Val loss: 0.106 | Gen: elay airway onditioningcay isway orkingway\n","Epoch:  39 | Train loss: 0.050 | Val loss: 0.293 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  40 | Train loss: 0.172 | Val loss: 0.200 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  41 | Train loss: 0.083 | Val loss: 0.133 | Gen: elay airway onditioningcay isway orkingway\n","Epoch:  42 | Train loss: 0.035 | Val loss: 0.098 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  43 | Train loss: 0.022 | Val loss: 0.094 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  44 | Train loss: 0.017 | Val loss: 0.091 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  45 | Train loss: 0.014 | Val loss: 0.090 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  46 | Train loss: 0.012 | Val loss: 0.089 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  47 | Train loss: 0.011 | Val loss: 0.090 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  48 | Train loss: 0.016 | Val loss: 0.100 | Gen: ewhay airway onditioningcay isway orkingway\n","Epoch:  49 | Train loss: 0.026 | Val loss: 0.116 | Gen: ewhay airway onditioningcay isway orkingway\n","Epoch:  50 | Train loss: 0.017 | Val loss: 0.102 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  51 | Train loss: 0.015 | Val loss: 0.115 | Gen: ewhay airway onditioningcay isway orkingway\n","Epoch:  52 | Train loss: 0.031 | Val loss: 0.148 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  53 | Train loss: 0.158 | Val loss: 0.254 | Gen: ethay airway onditioncgway isway orkingway\n","Epoch:  54 | Train loss: 0.085 | Val loss: 0.203 | Gen: elay airway onditioncingcay isway orkingway\n","Epoch:  55 | Train loss: 0.072 | Val loss: 0.164 | Gen: ethay airway onditioncgay isway orkingway\n","Epoch:  56 | Train loss: 0.031 | Val loss: 0.106 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  57 | Train loss: 0.019 | Val loss: 0.090 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  58 | Train loss: 0.011 | Val loss: 0.085 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  59 | Train loss: 0.008 | Val loss: 0.083 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  60 | Train loss: 0.007 | Val loss: 0.082 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  61 | Train loss: 0.006 | Val loss: 0.082 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  62 | Train loss: 0.006 | Val loss: 0.081 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  63 | Train loss: 0.005 | Val loss: 0.081 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  64 | Train loss: 0.005 | Val loss: 0.081 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  65 | Train loss: 0.004 | Val loss: 0.082 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  66 | Train loss: 0.004 | Val loss: 0.083 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  67 | Train loss: 0.004 | Val loss: 0.082 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  68 | Train loss: 0.004 | Val loss: 0.081 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  69 | Train loss: 0.003 | Val loss: 0.080 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  70 | Train loss: 0.003 | Val loss: 0.087 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  71 | Train loss: 0.003 | Val loss: 0.085 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  72 | Train loss: 0.003 | Val loss: 0.083 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  73 | Train loss: 0.002 | Val loss: 0.083 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  74 | Train loss: 0.002 | Val loss: 0.081 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  75 | Train loss: 0.002 | Val loss: 0.082 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  76 | Train loss: 0.002 | Val loss: 0.080 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  77 | Train loss: 0.002 | Val loss: 0.081 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  78 | Train loss: 0.002 | Val loss: 0.081 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  79 | Train loss: 0.002 | Val loss: 0.079 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  80 | Train loss: 0.002 | Val loss: 0.084 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  81 | Train loss: 0.003 | Val loss: 0.116 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  82 | Train loss: 0.217 | Val loss: 0.342 | Gen: eway airway onditioningcay isway orkingway\n","Epoch:  83 | Train loss: 0.159 | Val loss: 0.189 | Gen: eplay airway onditioncingcay isway orkingway\n","Epoch:  84 | Train loss: 0.046 | Val loss: 0.155 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  85 | Train loss: 0.024 | Val loss: 0.112 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  86 | Train loss: 0.011 | Val loss: 0.104 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  87 | Train loss: 0.008 | Val loss: 0.103 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  88 | Train loss: 0.007 | Val loss: 0.101 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  89 | Train loss: 0.005 | Val loss: 0.101 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  90 | Train loss: 0.005 | Val loss: 0.100 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  91 | Train loss: 0.004 | Val loss: 0.099 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  92 | Train loss: 0.004 | Val loss: 0.098 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  93 | Train loss: 0.003 | Val loss: 0.097 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  94 | Train loss: 0.003 | Val loss: 0.096 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  95 | Train loss: 0.003 | Val loss: 0.095 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  96 | Train loss: 0.003 | Val loss: 0.095 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  97 | Train loss: 0.002 | Val loss: 0.094 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  98 | Train loss: 0.002 | Val loss: 0.094 | Gen: ethay airway onditioningcay isway orkingway\n","Epoch:  99 | Train loss: 0.002 | Val loss: 0.094 | Gen: ethay airway onditioningcay isway orkingway\n","source:\t\tthe air conditioning is working \n","translated:\tethay airway onditioningcay isway orkingway\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VNVKbLc0ACj_","colab_type":"code","outputId":"fff3c67e-16fd-4a92-8a83-04707ba12eb5","executionInfo":{"status":"ok","timestamp":1584574329405,"user_tz":240,"elapsed":1064259,"user":{"displayName":"Frank Chen","photoUrl":"","userId":"12545694808850748741"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["TEST_SENTENCE = 'jcascs'\n","translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["source:\t\tjcascs \n","translated:\tascsjay\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kw_GOIvzo1ix","colab_type":"text"},"source":["# Part 3: Scaled Dot Product Attention"]},{"cell_type":"markdown","metadata":{"id":"xq7nhsEio1w-","colab_type":"text"},"source":["## Step 1: Implement Dot-Product Attention\n","Implement the scaled dot product attention module described in the assignment worksheet. "]},{"cell_type":"code","metadata":{"id":"d_j3oY3hqsJQ","colab_type":"code","colab":{}},"source":["class ScaledDotAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(ScaledDotAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","\n","        self.Q = nn.Linear(hidden_size, hidden_size)\n","        self.K = nn.Linear(hidden_size, hidden_size)\n","        self.V = nn.Linear(hidden_size, hidden_size)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the scaled dot attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x k x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","\n","            The output must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","\n","        batch_size = keys.size()[0]\n","        q = self.Q(queries).view(batch_size, -1, self.hidden_size)\n","        k = self.K(keys)\n","        v = self.V(values)\n","        unnormalized_attention = torch.bmm(k, q.transpose(1,2)) * self.scaling_factor                                  \n","        attention_weights = self.softmax(unnormalized_attention)\n","        context = torch.bmm(attention_weights.transpose(1,2), v)\n","        return context, attention_weights\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"unReAOrjo113","colab_type":"text"},"source":["## Step 2: Implement Causal Dot-Product Attention\n","Now implement the scaled causal dot product described in the assignment worksheet. "]},{"cell_type":"code","metadata":{"id":"ovigzQffrKqj","colab_type":"code","colab":{}},"source":["class CausalScaledDotAttention(nn.Module):\n","    def __init__(self, hidden_size):\n","        super(CausalScaledDotAttention, self).__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.neg_inf = torch.tensor(-1e7)\n","\n","        self.Q = nn.Linear(hidden_size, hidden_size)\n","        self.K = nn.Linear(hidden_size, hidden_size)\n","        self.V = nn.Linear(hidden_size, hidden_size)\n","        self.softmax = nn.Softmax(dim=1)\n","        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n","\n","    def forward(self, queries, keys, values):\n","        \"\"\"The forward pass of the scaled dot attention mechanism.\n","\n","        Arguments:\n","            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n","            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n","\n","        Returns:\n","            context: weighted average of the values (batch_size x k x hidden_size)\n","            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n","\n","            The output must be a softmax weighting over the seq_len annotations.\n","        \"\"\"\n","        batch_size = keys.size()[0]\n","        q = self.Q(queries).view(batch_size, -1, self.hidden_size)\n","        k = self.K(keys)\n","        v = self.V(values)\n","        unnormalized_attention = torch.bmm(k, q.transpose(1,2)) * self.scaling_factor\n","        mask = self.neg_inf * torch.ones_like(unnormalized_attention).tril(1)\n","        attention_weights = self.softmax(unnormalized_attention + mask)\n","        context = torch.bmm(attention_weights.transpose(1,2), v)\n","        return context, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9tcpUFKqo2Oi","colab_type":"text"},"source":["## Step 3: Transformer Encoder\n","Complete the following transformer encoder implementation. "]},{"cell_type":"code","metadata":{"id":"N3B-fWsarlVk","colab_type":"code","colab":{}},"source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n","        super(TransformerEncoder, self).__init__()\n","\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.opts = opts\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)\n","        \n","        # IMPORTANT CORRECTION: NON-CAUSAL ATTENTION SHOULD HAVE BEEN\n","        # USED IN THE TRANSFORMER ENCODER. \n","        # NEW VERSION: \n","        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n","                                    hidden_size=hidden_size, \n","                                 ) for i in range(self.num_layers)])\n","        # PREVIONS VERSION: \n","        # self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n","        #                             hidden_size=hidden_size, \n","        #                          ) for i in range(self.num_layers)])\n","        self.attention_mlps = nn.ModuleList([nn.Sequential(\n","                                    nn.Linear(hidden_size, hidden_size),\n","                                    nn.ReLU(),\n","                                 ) for i in range(self.num_layers)])\n","\n","        self.positional_encodings = self.create_positional_encodings()\n","\n","    def forward(self, inputs):\n","        \"\"\"Forward pass of the encoder RNN.\n","\n","        Arguments:\n","            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n","\n","        Returns:\n","            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n","            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n","        \"\"\"\n","\n","        batch_size, seq_len = inputs.size()\n","       \n","        # batch_size x seq_len x hidden_size\n","        encoded = self.embedding(inputs)  \n","\n","        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n","        encoded += self.create_positional_encodings(seq_len).unsqueeze(0)\n","\n","        annotations = encoded\n","\n","        for i in range(self.num_layers):\n","          # batch_size x seq_len x hidden_size\n","          new_annotations, self_attention_weights = self.self_attentions[i](annotations, annotations, annotations) \n","          residual_annotations = annotations + new_annotations\n","          new_annotations = self.attention_mlps[i](residual_annotations)\n","          annotations = residual_annotations + new_annotations\n","      \n","        # Transformer encoder does not have a last hidden layer. \n","        return annotations, None  \n","\n","    def create_positional_encodings(self, max_seq_len=1000):\n","      \"\"\"Creates positional encodings for the inputs.\n","\n","      Arguments:\n","          max_seq_len: a number larger than the maximum string length we expect to encounter during training\n","\n","      Returns:\n","          pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n","      \"\"\"\n","      pos_indices = torch.arange(max_seq_len)[..., None]\n","      dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n","      exponents = (2*dim_indices).float()/(self.hidden_size)\n","      trig_args = pos_indices / (10000**exponents)\n","      sin_terms = torch.sin(trig_args)\n","      cos_terms = torch.cos(trig_args)\n","\n","      pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n","      pos_encodings[:, 0::2] = sin_terms\n","      pos_encodings[:, 1::2] = cos_terms\n","\n","      if self.opts.cuda:\n","        pos_encodings = pos_encodings.cuda()\n","\n","      return pos_encodings\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z1hDi020rT36","colab_type":"text"},"source":["## Step 4: Transformer Decoder\n","Complete the following transformer decoder implementation. "]},{"cell_type":"code","metadata":{"id":"nyvTZFxtrvc6","colab_type":"code","colab":{}},"source":["class TransformerDecoder(nn.Module):\n","    def __init__(self, vocab_size, hidden_size, num_layers):\n","        super(TransformerDecoder, self).__init__()\n","        self.vocab_size = vocab_size\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n","        self.num_layers = num_layers\n","        \n","        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n","                                    hidden_size=hidden_size, \n","                                 ) for i in range(self.num_layers)])\n","        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n","                                    hidden_size=hidden_size, \n","                                 ) for i in range(self.num_layers)])\n","        self.attention_mlps = nn.ModuleList([nn.Sequential(\n","                                    nn.Linear(hidden_size, hidden_size),\n","                                    nn.ReLU(),\n","                                 ) for i in range(self.num_layers)])\n","        self.out = nn.Linear(hidden_size, vocab_size)\n","\n","        self.positional_encodings = self.create_positional_encodings()\n","\n","    def forward(self, inputs, annotations, hidden_init):\n","        batch_size, seq_len = inputs.size()\n","        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size \n","\n","        # THIS LINE WAS ADDED AS A CORRECTION. \n","        embed = embed + self.positional_encodings[:seq_len]       \n","\n","        encoder_attention_weights_list = []\n","        self_attention_weights_list = []\n","        contexts = embed\n","        for i in range(self.num_layers):\n","          \n","          # batch_size x seq_len x hidden_size\n","          new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts) \n","          residual_contexts = contexts + new_contexts\n","\n","          # batch_size x seq_len x hidden_size\n","          new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations)\n","          residual_contexts = residual_contexts + new_contexts\n","          new_contexts = self.attention_mlps[i](residual_contexts)\n","          contexts = residual_contexts + new_contexts\n","        \n","          encoder_attention_weights_list.append(encoder_attention_weights)\n","          self_attention_weights_list.append(self_attention_weights)\n","          \n","        output = self.out(contexts)\n","        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n","        self_attention_weights = torch.stack(self_attention_weights_list)\n","        \n","        return output, (encoder_attention_weights, self_attention_weights)\n","\n","    def create_positional_encodings(self, max_seq_len=1000):\n","      \"\"\"Creates positional encodings for the inputs.\n","\n","      Arguments:\n","          max_seq_len: a number larger than the maximum string length we expect to encounter during training\n","\n","      Returns:\n","          pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n","      \"\"\"\n","      pos_indices = torch.arange(max_seq_len)[..., None]\n","      dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n","      exponents = (2*dim_indices).float()/(self.hidden_size)\n","      trig_args = pos_indices / (10000**exponents)\n","      sin_terms = torch.sin(trig_args)\n","      cos_terms = torch.cos(trig_args)\n","\n","      pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n","      pos_encodings[:, 0::2] = sin_terms\n","      pos_encodings[:, 1::2] = cos_terms\n","\n","      pos_encodings = pos_encodings.cuda()\n","\n","      return pos_encodings"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"29ZjkXTNrUKb","colab_type":"text"},"source":["\n","## Step 5: Training and analysis\n","Now, train the following language model that's comprised of a (simplified) transformer encoder and transformer decoder. "]},{"cell_type":"code","metadata":{"id":"SmoTgrDcr_dw","colab_type":"code","outputId":"496273ec-ac40-4c72-a59a-d83264ac4aaa","executionInfo":{"status":"ok","timestamp":1584574589844,"user_tz":240,"elapsed":1324653,"user":{"displayName":"Frank Chen","photoUrl":"","userId":"12545694808850748741"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["TEST_SENTENCE = 'the air conditioning is working'\n","\n","args = AttrDict()\n","args_dict = {\n","              'cuda':True, \n","              'nepochs':100, \n","              'checkpoint_dir':\"checkpoints\", \n","              'learning_rate':0.005,  ## INCREASE BY AN ORDER OF MAGNITUDE\n","              'lr_decay':0.99,\n","              'batch_size':64, \n","              'hidden_size':20, \n","              'encoder_type': 'transformer',\n","              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n","              'num_transformer_layers': 3,\n","}\n","args.update(args_dict)\n","\n","print_opts(args)\n","transformer_encoder, transformer_decoder = train(args)\n","\n","translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["================================================================================\n","                                      Opts                                      \n","--------------------------------------------------------------------------------\n","                            hidden_size: 20                                     \n","                          learning_rate: 0.005                                  \n","                 num_transformer_layers: 3                                      \n","                             batch_size: 64                                     \n","                                nepochs: 100                                    \n","                           encoder_type: transformer                            \n","                                   cuda: 1                                      \n","                         checkpoint_dir: checkpoints                            \n","                           decoder_type: transformer                            \n","                               lr_decay: 0.99                                   \n","================================================================================\n","================================================================================\n","                                   Data Stats                                   \n","--------------------------------------------------------------------------------\n","('payment', 'aymentpay')\n","('ordination', 'ordinationway')\n","('amends', 'amendsway')\n","('principally', 'incipallypray')\n","('anybody', 'anybodyway')\n","Num unique word pairs: 6387\n","Vocabulary: ['EOS', '-', 'SOS', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z']\n","Vocab size: 29\n","================================================================================\n","Moved models to GPU!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python2.7/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type TransformerEncoder. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type ScaledDotAttention. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python2.7/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type TransformerDecoder. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch:   0 | Train loss: 2.290 | Val loss: 1.754 | Gen:   -------------------- -------------------- --------\n","Epoch:   1 | Train loss: 1.629 | Val loss: 1.532 | Gen: e  onininininininininin  \n","Epoch:   2 | Train loss: 1.463 | Val loss: 1.393 | Gen: e iy iiiiiiiiiiiioiowy iy oieeiiiooooowway\n","Epoch:   3 | Train loss: 1.251 | Val loss: 1.077 | Gen: ----- - - - -       \n","Epoch:   4 | Train loss: 0.838 | Val loss: 0.636 | Gen:   --------------------  --------------------\n","Epoch:   5 | Train loss: 0.521 | Val loss: 0.479 | Gen:                     \n","Epoch:   6 | Train loss: 0.319 | Val loss: 0.229 | Gen: ewwwwwwwwwwwwwwwwwww rarrrrrrrrrrrrrrrrrr stttttttttttttttttst eeeeeeeeeeeeeeeeeeee saay\n","Epoch:   7 | Train loss: 0.164 | Val loss: 0.140 | Gen: - - - d -           \n","Epoch:   8 | Train loss: 0.088 | Val loss: 0.094 | Gen: -wwwwwwwwwwwwwwwwwwa wwwwwwwwwwwwwwwwwwww -iwwwiliiwalaiwwwwal iddddddddddddddddddd -wwwwwwwwwwwwwwwwwwa\n","Epoch:   9 | Train loss: 0.146 | Val loss: 0.136 | Gen:    dddddddddddddddddddd \n","Epoch:  10 | Train loss: 0.065 | Val loss: 0.049 | Gen: --- --------- i-iii-- --- ---------\n","Epoch:  11 | Train loss: 0.040 | Val loss: 0.129 | Gen:                     \n","Epoch:  12 | Train loss: 0.094 | Val loss: 0.040 | Gen: rway rllllllllay rrrrrrrrrrrrrrrrrxpr ssssssssssssssssssss rlllwllllwwlllllwwll\n","Epoch:  13 | Train loss: 0.068 | Val loss: 0.016 | Gen: --------- e-EOSEOSn eeirrnnnny ssssssssssssssss ---------------\n","Epoch:  14 | Train loss: 0.014 | Val loss: 0.018 | Gen: ossssssssssssssss esnnsssrgpgssssgnd e--------EOSEOSEOSEOSEOS- sssssssssssssssss ossssssrsssssssss\n","Epoch:  15 | Train loss: 0.027 | Val loss: 0.007 | Gen: ossssssssssssssssdds srgggggggnnnggnnnwdg ossnnnnnnnnnnnnnn sssssssssssssssssdss mrrrrrrrrrrrrrrr\n","Epoch:  16 | Train loss: 0.018 | Val loss: 0.004 | Gen: --- --- --- -------- ---\n","Epoch:  17 | Train loss: 0.008 | Val loss: 0.003 | Gen: oay - - wayy -      \n","Epoch:  18 | Train loss: 0.062 | Val loss: 0.062 | Gen: -eeeeeeeeeeeeeeey eeeeeeeeeeeeeeeee eeeeeeeeeeeeeeeey eeeeeeeeeeeeeeeee -eeeeeeeeeeeeeeey\n","Epoch:  19 | Train loss: 0.030 | Val loss: 0.004 | Gen:   eeeeeeeeeeeeeeeeeeee osssssssssssssssssss \n","Epoch:  20 | Train loss: 0.108 | Val loss: 0.075 | Gen: ---- ---vv----- lllttccfficccccccccc --------------- -vhivcvcffffn-ff\n","Epoch:  21 | Train loss: 0.027 | Val loss: 0.005 | Gen:  iiidieiiididiiiidddi  iiidi---- \n","Epoch:  22 | Train loss: 0.027 | Val loss: 0.012 | Gen: eeeeeeeeeeeeeeeeeeee ---- ---- ---- eeeeeeeeeeeeeeeeeeee\n","Epoch:  23 | Train loss: 0.007 | Val loss: 0.003 | Gen:  say                \n","Epoch:  24 | Train loss: 0.044 | Val loss: 0.014 | Gen: ---------------- ---------------- ---------------- ---------------- ----------------\n","Epoch:  25 | Train loss: 0.014 | Val loss: 0.006 | Gen: -- -  - --          \n","Epoch:  26 | Train loss: 0.007 | Val loss: 0.006 | Gen: -------------------- -------------------- ---------------- -------------------- ----------------\n","Epoch:  27 | Train loss: 0.025 | Val loss: 0.006 | Gen: - dddddddddddddddiiddd  - \n","Epoch:  28 | Train loss: 0.012 | Val loss: 0.012 | Gen: --------------- annnnnnnnnnnnnnn annnnnnnnnnccnncccca ---------------- annnnnnnnnncnnccccca\n","Epoch:  29 | Train loss: 0.012 | Val loss: 0.003 | Gen: -- -- -- -- --      \n","Epoch:  30 | Train loss: 0.004 | Val loss: 0.001 | Gen: -- -- -- -- --      \n","Epoch:  31 | Train loss: 0.003 | Val loss: 0.001 | Gen: -- -- -- -- --      \n","Epoch:  32 | Train loss: 0.003 | Val loss: 0.001 | Gen: -- -- -- -- --      \n","Epoch:  33 | Train loss: 0.002 | Val loss: 0.001 | Gen: -- -- -- -- --      \n","Epoch:  34 | Train loss: 0.002 | Val loss: 0.001 | Gen: -- -- -- -- --      \n","Epoch:  35 | Train loss: 0.002 | Val loss: 0.001 | Gen: -- -- -- -- --      \n","Epoch:  36 | Train loss: 0.002 | Val loss: 0.001 | Gen: -- -- -- --- --     \n","Epoch:  37 | Train loss: 0.002 | Val loss: 0.001 | Gen:   annnnnnnnnnnnnnn  annnnnnnnnnnnnnn\n","Epoch:  38 | Train loss: 0.008 | Val loss: 0.001 | Gen: --- annnnnnnnnnnnnnn annnnnnnnnnnnnnn --- annnnnnnnnccnnnnccca\n","Epoch:  39 | Train loss: 0.005 | Val loss: 0.003 | Gen: --- --- -- --- --   \n","Epoch:  40 | Train loss: 0.002 | Val loss: 0.001 | Gen:  -                  \n","Epoch:  41 | Train loss: 0.005 | Val loss: 0.001 | Gen: --- -- --- --- ---  \n","Epoch:  42 | Train loss: 0.003 | Val loss: 0.001 | Gen: - - annnnnnnnnnnnnnn - annnnnnnnnnnnnnn\n","Epoch:  43 | Train loss: 0.295 | Val loss: 0.293 | Gen: annnnnnnnnnnnnnnc auppy ay aiiiiiiiiiiiiiiiidii auppyyyy\n","Epoch:  44 | Train loss: 0.073 | Val loss: 0.022 | Gen: ---------------- ---------------- ----------------- ---- ----------------\n","Epoch:  45 | Train loss: 0.020 | Val loss: 0.007 | Gen: oooooooooooooooooooo oooooooooooooooooooo oooooooooooooooooaeo oooooooooooooooooaeo oooooooooooooooooaeo\n","Epoch:  46 | Train loss: 0.021 | Val loss: 0.004 | Gen: azeemmmmmemmssssshea ay annnpppppppppppppppp annnnnppppppppppssps annnccccccccccccccca\n","Epoch:  47 | Train loss: 0.008 | Val loss: 0.003 | Gen: annnppppppppppppppps ay ay aaaaaaaaaaaiaaaaaiaa annnnnnnnnncnnnnncca\n","Epoch:  48 | Train loss: 0.012 | Val loss: 0.003 | Gen: annnccccccccccccccca ay ay aaaaaaaaaaaaaaaaaaaa aalaaaay\n","Epoch:  49 | Train loss: 0.017 | Val loss: 0.002 | Gen: ay ay ay aaaaaaaaaaaaaaaaaiaa allaaayllllllllllaay\n","Epoch:  50 | Train loss: 0.004 | Val loss: 0.001 | Gen: annnnnnnnnsnnnnnspra ay annnnppppppppppppppp annnnnnnnnsnnnnnsx annnnnnnnnspnnnnspra\n","Epoch:  51 | Train loss: 0.003 | Val loss: 0.001 | Gen: ay ay ay annnnnnnnnsnnnnnsx annnnnnnnnncnnnnccca\n","Epoch:  52 | Train loss: 0.002 | Val loss: 0.001 | Gen: annnnnnnnnsnnnnnspra ay ay aaaaaaaaaaaiaaaaaiaa annnnnnnnnncnnnnccca\n","Epoch:  53 | Train loss: 0.001 | Val loss: 0.001 | Gen: annnnnnnnnsnnnnnspra ay ay annnnnnnnnsnnnnnsx annnnnnnnnncnnnnncca\n","Epoch:  54 | Train loss: 0.002 | Val loss: 0.001 | Gen: ay ay ay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaay\n","Epoch:  55 | Train loss: 0.002 | Val loss: 0.001 | Gen: annnnnnnnnsnnnnnscca ay annnnnnnnnsnnnnnsppn annnnnnnnnsnnnnnsx annnnnnnnnncnnnnncca\n","Epoch:  56 | Train loss: 0.003 | Val loss: 0.001 | Gen: ay ay ay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaay\n","Epoch:  57 | Train loss: 0.002 | Val loss: 0.001 | Gen: annnnnnnnnsnnnnnscca ay ay aaaaaaaaaaaiaaaaaiaa ayaaaaaaeeeeaaaaaaaa\n","Epoch:  58 | Train loss: 0.001 | Val loss: 0.000 | Gen: ayaaayEOSrrlllrrrrray ay ay aaaaaaaaaaaiaaaaaiaa acaaaaaaaaaaaaaaaiaa\n","Epoch:  59 | Train loss: 0.001 | Val loss: 0.000 | Gen: ayaaayEOSrrlllrrrrray ay ay aaaaaaaaaaaiaaaaaiaa acaaaaaaaaaaaaaaaiaa\n","Epoch:  60 | Train loss: 0.000 | Val loss: 0.000 | Gen: ayaaayEOSrrlllrrrrray ay ay aaaaaaaaaaaiaaaaaiaa acaaaaaaaaaaaaaaaiaa\n","Epoch:  61 | Train loss: 0.000 | Val loss: 0.000 | Gen: ayaaayEOSrrlllrrrrray ay ay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaiaa\n","Epoch:  62 | Train loss: 0.000 | Val loss: 0.000 | Gen: ayaaayEOSrrlllrrrrray ay aay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  63 | Train loss: 0.000 | Val loss: 0.000 | Gen: ayaaayEOSrrlllrrrrray ay aay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  64 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaay ay aay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  65 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaay ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  66 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaay ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  67 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiaa ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  68 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiaa ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  69 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiaa ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  70 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiaa ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  71 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiaa ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  72 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiaa ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  73 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiaa ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  74 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiaa ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  75 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiaa ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  76 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiaa ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  77 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  78 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  79 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  80 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  81 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  82 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  83 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  84 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  85 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  86 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  87 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaayEOSaaaaaaaaiaa aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  88 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaayEOSaaaaaaaaiaa aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  89 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaayEOSaaaaaaaaiaa aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  90 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaayEOSaaaaaaaaiaa aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  91 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaayaaaaaaaaaiaa aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  92 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaayaaaaaaaaaiaa aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  93 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  94 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  95 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  96 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  97 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  98 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","Epoch:  99 | Train loss: 0.000 | Val loss: 0.000 | Gen: aaaaaaaaaaaaaaaaaiia ay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n","source:\t\tthe air conditioning is working \n","translated:\taaaaaaaaaaaaaaaaaiia ay aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaiaa aaaaaaaaaaaaaaaaaaaa\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R18s80gzC6A8","colab_type":"code","outputId":"fdee5477-bea8-499c-9811-b77da2cbcfb5","executionInfo":{"status":"error","timestamp":1584577125494,"user_tz":240,"elapsed":1118,"user":{"displayName":"Frank Chen","photoUrl":"","userId":"12545694808850748741"}},"colab":{"base_uri":"https://localhost:8080/","height":198}},"source":["TEST_SENTENCE = ''\n","translated = translate_sentence(TEST_SENTENCE, transformer_encoder, transformer_decoder, None, args)\n","print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-1-e5701212badd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTEST_SENTENCE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_SENTENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"source:\\t\\t{} \\ntranslated:\\t{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_SENTENCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'translate_sentence' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"MBnBXRG8mvcn","colab_type":"text"},"source":["# Optional: Attention Visualizations\n","\n","One of the benefits of using attention is that it allows us to gain insight into the inner workings of the model.\n","\n","By visualizing the attention weights generated for the input tokens in each decoder step, we can see where the model focuses while producing each output token.\n","\n","The code in this section loads the model you trained from the previous section and uses it to translate a given set of words: it prints the translations and display heatmaps to show how attention is used at each step."]},{"cell_type":"markdown","metadata":{"id":"JqEC0vN9mvpV","colab_type":"text"},"source":["## Step 1: Visualize Attention Masks\n","Play around with visualizing attention maps generated by the previous two models you've trained. Inspect visualizations in one success and one failure case for both models. "]},{"cell_type":"code","metadata":{"id":"Dkfz-u-MtudL","colab_type":"code","outputId":"85846013-589d-4e3c-8925-63804bcf6798","executionInfo":{"status":"ok","timestamp":1584557410765,"user_tz":240,"elapsed":615,"user":{"displayName":"Frank Chen","photoUrl":"","userId":"12545694808850748741"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["TEST_WORD_ATTN = 'street'\n","visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZAAAAEhCAYAAABRKfYcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["'eetstray'"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"Ssa7g35zt2yj","colab_type":"code","outputId":"02603a82-7014-464d-a2ed-a7ed90b77b18","executionInfo":{"status":"ok","timestamp":1584557415282,"user_tz":240,"elapsed":1095,"user":{"displayName":"Frank Chen","photoUrl":"","userId":"12545694808850748741"}},"colab":{"base_uri":"https://localhost:8080/","height":874}},"source":["TEST_WORD_ATTN = 'street'\n","visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAWMAAAEYCAYAAACJJ5fjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns1JwMTYzKwEXYzOzEnAxNjMrgf8PhaKe3dgBF6wAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAWMAAAEYCAYAAACJJ5fjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nO3deZwdZZ3v8c+3O0ASEppgAkIaBJX1\nqhACKG5ERI2OC44o4uBcZrzmjvuSBkEJRlYhreMGalDMqDA6rsNVNDhA1BFREhJCiGyiSAcEwhIT\n1qT7d/+oajj0dPc53U+dVHWf75vXeXGquupXv7P98pyn6jyPIgIzMytXW9kJmJmZi7GZWSW4GJuZ\nVYCLsZlZBbgYm5lVgIuxmVkFuBiPIZJ2lPTesvOoAj8XNt64GI8tOwIuQJnKPRfK+DNlo1LpN46k\n4yX9XtIqSV+V1N7KeQCfBp6T57GojAQkbS/pp5Kul7RG0rFl5EEFngsASXtKulnSN4E1wO5b+fin\nS/pwzfJZkj60NXOwYqiqv8CTtD9wHvD3EbFZ0gXANRHxzVbMI89lT+AnEfG8rX3smhzeAsyNiHfn\nyx0RsaGEPPak5OeiJo/bgRdHxDUlHf+HEXFw3iq/FTgsIu7f2rlYmgllJzCMVwKzgWslAUwC7m3h\nPKriBuAzks4lK4a/LjuhCrijjEIMEBF/lnS/pFnALsBKF+KxqcrFWMC/RcQpzqM6IuIWSQcDrwPO\nlHRFRJxedl4le7jk438NOAF4JnBRuanYaFW5z/gK4BhJOwNI2knSs1o4D4CNwNSSjg2ApN2ARyLi\n28Ai4OCSUin9uaiQHwFzgUOBpSXnYqNU2ZZxRKyVdCpwed4Xthl4H3BHK+aR53K/pN9IWgP8LCJO\n3No5AM8HFknqI3su3lNCDlV5LiohIp6QdBXwUET0lp2PjU5lT+CZWWPyRsJ1wFsj4tay87HRqXI3\nhZnVIekA4DbgChfisc0tYzOzCnDL2MysAlyMzcwqYEwUY0nzys4BqpFHFXIA51G1HKAaeVQhh7Fq\nTBRjoCovcBXyqEIO4DxqVSEHqEYeVchhTBorxdjMbFxr+tUUUlu0taXV/KlTp7Bx46akGH196dfC\nd3R0sGHDVh8Tp3I5jLc8Jk1K+yHfxIkTeOyxLUkxnrHLjKT9AR7btJGJU9IeS1t72mf1kb9tYPIO\nHUkx7r/3Xjb9bYOSggwwd+7cWL9+fcPbr1ixYmlEzC0yh3qa/gu8trY2Jk2akhTjk59cyIIFC5Ni\nPPxweuFYsGABXV1dyXHGeg7F5ZH+ecvySPvx3X77vShp/3e/+61ceOH3kmK840PvTtofYPcpE7hz\nU9o/CtvvmPZZnc5m1rNNUoxPz/9g0v6DWb9+PcuXL294e0nTC0+ijsr+HNrMrEhV/02Fi7GZtYQ+\nF2Mzs3IFbhmbmZUvgl4XYzOz8rllbGZWssB9xmZmleCWsZlZBbgYm5mVLCLcTWFmVgVuGZuZVUDg\nYmxmVqrsaoqysxiei7GZtYSqd1M0NF6epLdKmprfP1XSDyUd3NzUzMyK05efxGvkVoZGBy9dEBEb\nJb0UOAr4OvDl5qVlZlagCGIEtzI0NLi8pJURMUvSOcANEXFJ/7ohtp9HPv1KR0fH7E9+cmFSkjNn\n7sa6dXclxShicPnOzk56enqS44z1HMZbHpMn75C0//Tp01i//sGkGDvtnD587rbt4onetELSNqE9\naf8JBFsSx6numj+fO267tdDB5Q+cNSt+ftVVDW+/27RpKyLikCJzqKfRPuN1kr4KvAo4V9J2DNOq\njojFwGKA9vYJkTow/BlnVGNw+e7u7tIHdq9CDsXlkf556+5elDy4/KxZRyXtP74Gl5+YtH8Rg8s3\ny7joMwbeBiwFXhMRDwE7AWmfADOzrSZG9F8ZGmoZR8QjwA9rlu8G7m5WUmZmRYrwpW1mZpVQ9W4K\nF2MzawkuxmZmJfN4xmZmFeGWsZlZ2TyEpplZNbhlbGZWssBDaJqZVUJvxS80djE2s5bgbgozs5J5\nDjwzs4pwy9jMrAJcjM3MSuZf4JEN6v7oo5tKj2HVE9GXHGPZsmXJcV7z6n9O2n/ChG2ZMb0zKcbP\nvnlp0v4Axx57JD/77pVJMR5//JGk/d/5ztdx8bcuS4px/1/XJ+0/FF/aZmZWARW/ss3F2MxaQIlz\n2zXKxdjMxr3AJ/DMzCqh5U/gmZlVQdVbxo1OSGpmNmZFBL19fQ3f6pE0V9LNkm6TdPIgf99D0lWS\nVkpaLel19WK6GJtZSyhqdmhJ7cD5wGuBA4DjJB0wYLNTgf+IiFnA24EL6uXnYmxmLaEvGr/VcRhw\nW0TcHhFPAN8B3jRgmwB2yO93AHfVC+o+YzMb90ZxNcV0SctrlhdHxOL8/kzgzpq/9QAvHLD/QuBy\nSR8AtgeOqndAF2MzawkjLMbrI+KQhMMdByyJiM9IOhz4lqTnxTA/F3UxNrOWUOClbeuA3WuWO/N1\ntd4FzAWIiN9KmghMB+4dKqj7jM1s/Mt/gdforY5rgb0l7SVpW7ITdAMHF/kL8EoASfsDE4H7hgvq\nlrGZjXtF/gIvIrZIej+wFGgHLoqIGyWdDiyPiEuB+cCFkj6SH/6EqJOAi7GZtYQif4EXEZcBlw1Y\nd1rN/bXAS0YSs243haRzG1lnZlZlRV1n3CyN9Bm/apB1ry06ETOzZopo/FYGDdWNIek9wHuBZwN/\nrPnTVOA3EXH8kEGlecA8gI6OjtkLFixISrKzs5Oenp6kGEWoQh5VyKGoPGbPnp2cx6ZNm5gyZUpS\njFtvvSNp/2nTpvDgg2mTHxQx0P5OO+3AAw/8rdQ8nvGMDu6/f0NSjPnzu9i06UElBRlg7wMOiM9d\ncknD279+1qwViZe2jdhwfcaXAD8DzgFqf3u9MSIeGC5ofnH0YgBJcdJJH0tK8rzzziU1Rl9fb9L+\nAN3d3XR1dSXHGes5FJVHESdUli1bxpw5c5JinHN22kwfx7z1CL7/vV8mxdjSuzlpf8hm+vhuBWb6\n+FbiTB/NMmZHbYuIDcAGsouXzczGLI9nbGZWES7GZmYVMGa7KczMxo/yLllrlIuxmY17ZV6y1igX\nYzNrCe6mMDOrAJ/AMzMrWeCWsZlZJbhlbGZWtsbGKS6Vi7GZtYToTR//o5lcjM2sJVS8YexibGbj\nX3adcbWrsYuxmbUEF2OKeRKq/kTayLW1tSfHWLToPI488pVJMd7blTZxzcTJE9n7oAOSYnz3G59P\n2h/gjW88jNWrlyXF6O3dkrT/o48ewdq1VyfFeOyxtLGhB+cTeGZmlRB9LsZmZqVyn7GZWUW4GJuZ\nVYGLsZlZ+Spei12MzawFRPgEnplZ2QLo6/PPoc3MSucTeGZmFeBibGZWtghwn7GZWfncMjYzq4CK\n12IXYzMb/4Jx0DKWJKAzIu7cCvmYmRVvDIxN0VZvg8gewWVbIRczs6aJvmj4Voa6xTh3naRDm5qJ\nmVnTZOMZN3orgxo5sKSbgOcCdwAPAyJrNL9giO3nAfMAOjo6Zi9YsCApyc7OTnp6epJiFKEKeVQh\nh/GWx87P7Ezaf/tJ2/Lwo08kxXhg/b1J+wPsuusu3H33PUkxUgvRbrs9k7vu+mtSjK6u+WzZsllJ\nQQbY4znPjflnf6bh7T/89qNXRMQhReZQT6Mn8F4zkqARsRhYDCApTjzxpJHm9TSLFp1HaoyI9J9C\ndnd309XVlRxnrOdQVB5So1/MhlbEeyN1po8XHrg7v7s+7ZRKETN9nHrqSZx55nlJMVJn+jjttFM4\n/fRzkmI0QwTEePg5dETc0exEzMyaqYD2WFP50jYzawlj/moKM7MxbwQn7xo8jzZX0s2SbpN08hDb\nvE3SWkk3SrqkXky3jM2sJRTVMpbUDpwPvAroAa6VdGlErK3ZZm/gFOAlEfGgpJ3rxXXL2MzGvf5f\n4BXUMj4MuC0ibo+IJ4DvAG8asM27gfMj4kGyY9e9XMbF2MzGvxjxjz6mS1pec5tXE20mUHv5TE++\nrtY+wD6SfiPpGklz66Xobgozaw0j66ZYn3id8QRgb2AO0An8StLzI+KhoXZwy9jMWkChJ/DWAbvX\nLHfm62r1AJdGxOaI+BNwC1lxHpKLsZm1hIjGb3VcC+wtaS9J2wJvBy4dsM2PyVrFSJpO1m1x+3BB\n3U1hZi2hqKspImKLpPcDS4F24KKIuFHS6cDyiLg0/9urJa0FeoETI+L+4eK6GJvZuBf5Cbzi4sVl\nDBjNMiJOq7kfwEfzW0NcjM2sJfSNh7EpzMzGtvKGxmyUi7GZjX9jYKYPF2Mzaw0lzeDRqKYX4z33\n2YdPfWVxUoztN21kyRVXJsX4P68Z0ZDMg5LENttsN+r9N29+PDmH8aSIMaaLiHP+ohOT9t+ruzs5\nRltbe9L+AFu2PMEDD9ydFKOvrzdp/97ezTz4YNrg8s2Q/Ry67CyG55axmbUEd1OYmZWtxLntGuVi\nbGYtoaxZnxvlYmxmLcEtYzOzkvWPZ1xlLsZmNv6NgcspXIzNrAUEfb0uxmZmpXM3hZlZ2fxzaDOz\n8vkEnplZRbgYm5mVLvyjDzOz0rnP2MysIipejBuaHVqZ4yWdli/vIemw5qZmZlacAmeHboqGijFw\nAXA4cFy+vBE4vykZmZkVrP9qikZvZVAjB5Z0XUQcLGllRMzK110fEQcOsf08YB7A9BkzZn/l6xcl\nJdne10tv4uDbd9x6S9L+ADNnzmTdunWj3r+IF7mzs5Oenp7kOM5jfOVQlTyKyKGrq4uIUEEpAbDL\nrnvEO941v+HtP3fWh1dExCFF5lBPo33GmyW1k/0Dg6QZwJDTK0TEYmAxwF777hsPT5malOT2mzaS\nGuPkkz+etD/Apz99dlKcImb66O7upqurKzmO86hWDkXM9HHeeedy0kkfS4qROtNHFV6PwY2f8Yy/\nAPwI2FnSWcAxwKlNy8rMrGB9fcVM89UsDRXjiLhY0grglYCAoyPiD03NzMysSOOkZUxE3ATc1MRc\nzMyaIsIzfZiZVULFG8YuxmbWCsbPCTwzszHNxdjMrGwem8LMrHyBT+CZmVWCW8ZmZqUrcQSgBrkY\nm9n45z5jM7Nq6Ot1MTYzK5UnJDUzqwJ3U5iZVYF/gcc9d97N5z56ZlKMd897Gxcu/nxSjG9ceUXS\n/gCTHnooKc77Xv/m5Bza2yfQ0TEjKcbGjQ8k5wHpY/Cmjp07nhT1XPg5HVrVi3Gj0y6ZmY1p0RcN\n3+qRNFfSzZJuk3TyMNu9RVJIqjtriIuxmY1/2Rm8QmYkzWc9Oh94LXAAcJykAwbZbirwIeB3jaTo\nYmxm416BtRjgMOC2iLg9Ip4AvgO8aZDtzgDOBR5rJEcXYzNrCSOcHXq6pOU1t3k1oWYCd9Ys9+Tr\nniTpYGD3iPhpo/n5agozawEjvppi/Whnh5bUBnwWOGEk+7kYm9n4V+y0S+uA3WuWO/N1/aYCzwOW\nSQJ4JnCppDdGxPKhgroYm1lLKPDStmuBvSXtRVaE3w68o+Y4G4Dp/cuSlgFdwxVicDE2sxYQQF9f\nXzGxIrZIej+wFGgHLoqIGyWdDiyPiEtHE9fF2MxaQLFDaEbEZcBlA9adNsS2cxqJ6WJsZuNfQBTT\nMG4aF2MzawlV/zm0i7GZtQQXYzOzknk8YzOzKhgD4xk39HNoZY6XdFq+vIekw5qbmplZURofsa3A\nH4eMSKNjU1wAHA4cly9vJBu1yMxsbChwpKBmUCNNd0nXRcTBklZGxKx83fURceAQ288D5gFMmzZt\n9llnnZuU5PTp01i//sGkGLvsvlvS/gBtvVvoax99z85fbrstOYfddtuVu+66OylGb++W5Dw6Ozvp\n6elJjjMe8qhCDlXJo4gcurq6iAgVlBIA06btEnPmHFd/w9yPf/z5FaMdm2K0Gq0sm/MxPANA0gxg\nyKv2ImIxsBhg8uQd4sLF/5GUZDbTR1qM+V9cmLQ/ZDN9PLrjjqPef+HCs5JzWLjwE8lxipjp47zz\nzuWkkz6WFKOIWSm6u7vp6upKjjPWc6hKHlXIYTAxBvqMGy3GXwB+BOws6SzgGODUpmVlZlaoqPyU\nVA0V44i4WNIK4JWAgKMj4g9NzczMrEDjpWVMRNwE3NTEXMzMmmbcFGMzs7Eqm8Gj2oNTuBibWWtw\ny9jMrHyBi7GZWencZ2xmVgEuxmZmpfMJPDOz0o2nX+CZmY1pLsZmZqULoqDZoZvFxdjMWkIMPbZZ\nJbgYm1lLcDeFmVnJfAIPePTRjaxc9V9JMR55ZG5yjNP+931J+wN85CPz+Nd//cSo919ze/o4S39Y\nuTI5zn57PDs5j7a2diZNmpIU4+GHNyTnYdaYcDE2M6sCX2dsZlYBbhmbmVWAi7GZWdlKnPW5US7G\nZjbuBR5C08ysEnwCz8ysdL60zcysEvo8NoWZWbmy83cuxmZmJXM3hZlZNbgYm5mVz5e2mZlVQNW7\nKdrqbSDp3EbWmZlVVzYhaaO3MtQtxsCrBln32qITMTNrlv7xjBu91SNprqSbJd0m6eRB/v5RSWsl\nrZZ0haRn1Y051IElvQd4L/Bs4I81f5oK/CYijh8m0XnAPICOjo7ZCxYsqJfHsDo7O+np6UmKsd12\nk5L2B9hllxncc8/ox0Xed/99k3N47JFHmDh5clKMNatvSM5j5szdWLfurqQYfX29yXkU8d4YDzlU\nJY8icujq6iIiVFBKAEyevEPsu+9hDW+/atUVKyLikMH+JqkduIWsodoDXAscFxFra7Z5BfC7iHgk\nr6VzIuLY4Y45XDHuAKYB5wC1lX9jRDzQ6IOSlNxR093dTVdXV1KMZz/7wNQ08sHlF496/19ee2Vy\nDn9YuZL9Z81KilHE4PJnnLGQBQsWJsUoYnD5It4b4yGHquRRVA7NKMb77HNow9tff/2VwxXjw4GF\nEfGafPkUgIg4Z4jtZwFfioiXDHfMIU/gRcQGYANwXGPpm5lVV4En8GYCd9Ys9wAvHGb7dwE/qxfU\nV1OYWQsIIkbULTZd0vKa5cURMeKvxZKOBw4Bjqi3rYuxmY17o5iQdP1Q3RTAOmD3muXOfN3TSDoK\n+ARwREQ8Xu+ALsZm1hIK7Ka4Fthb0l5kRfjtwDtqN8j7ib8KzI2IexsJ6mJsZi0gCrt+OCK2SHo/\nsBRoBy6KiBslnQ4sj4hLgUXAFOB7kgD+EhFvHC6ui7GZtYQif4EXEZcBlw1Yd1rN/aNGGtPF2Mxa\nQtV/Du1ibGbj3ihO4G11LsZm1gI8O7SZWSUEnunDzKx07qYwM6sAF2Mzs9J5Djwzs9JFFDNkazO5\nGJtZS3DLGIAihiZNi7Fp00PJGfT29ibFee6unck5nH3OWbzhdW9IivHXB9cn57HimmvouTdtcPlp\n22+fnIdZY3xpm5lZJXh2aDOzCihrotFGuRib2bjnn0ObmVWCL20zM6sEF2MzswpwMTYzqwCfwDMz\nK1v4OmMzs9IF0OeWsZlZ+dxNYWZWOl/aZmZWCS7GZmYl8y/wzMwqwsXYzKx0ARU/gdc23B8lHSrp\nmTXL/yjpPyV9QdJOzU/PzKwYMYL/yqDhmu6SrgOOiogHJL0c+A7wAeAgYP+IOGaI/eYB8wA6Ojpm\nL1iwICnJzs5Oenp6kmJMmLBt0v4Au+66C3fffc+o9+/t3ZKcw8yZM1m3bl1SjAMPOjA5j0cefpjJ\niYPDr1q5MjmPIt4b4yGHquRRRA5dXV1ERBEzUjxpwoRtYurUxtuPDz1074qIOKTIHOqpV4yvj4gD\n8/vnA/dFxMJ8eVVEHFT3AFKkztLR3b2Irq4Tk2LsvPMeSfsDfPzj8zn77M+Mev8ND92bnMPZ55zF\nx0/5RFKMomb6mP2iFyXFKGKmj+7ubrq6upLjjPUcqpJHUTk0oxhPmTKt4e03bLhvqxfjYbspgHZJ\n/f3KrwSurPmb+5vNbEyICCL6Gr6VoV5B/Xfgl5LWA48CvwaQ9FxgQ5NzMzMrzJi+miIizpJ0BbAr\ncHk89WjayPqOzczGhL6+al9NUberISKukfQK4J8kAdwYEVc1PTMzsyKN5ZaxpJnAD4HHgBX56rdK\nOhd4c0SkndY3M9sqgmBst4y/BHw5IpbUrpT0j8AFwJualJeZWWHGws+h611NccDAQgwQEd8E9mtK\nRmZmTZBdUdHYrQz1WsaDFmtJbUB78emYmTXHWG8Z/0TShZKevDo/v/8V4LKmZmZmVpjGW8VlFe16\nxfgksuuJ75C0QtIK4M/A34Dyf3JkZtagMf2jj4jYDHRJWgA8N1/9x4h4pOmZmZkVZMyfwJN0EkBE\nPArsFxE39BdiSWdvhfzMzIrRP0N0I7cS1OumeHvN/VMG/G1uwbmYmTXJSAbQrObVFBri/mDLZmaV\n1dfXW3YKw6pXjGOI+4Mtm5lVVtX7jOuNZ9wLPEzWCp4E9J+4EzAxIrapewDpPuCOxDynA+mD8Kar\nQh5VyAGcR9VygGrkUUQOz4qIGUUk00/Sz8lya9T6iNiqXbHDFuOqkLR8aw/0XNU8qpCD86heDlXJ\nowo5jFX1TuCZmdlW4GJsZlYBY6UYLy47gVwV8qhCDuA8alUhB6hGHlXIYUyqRDGWdLSkkLRfzbqD\nJL0OICIWS5oj6cUJx9hR0ntrlneT9P2RxIiIQt9okv4lH450uG1OkPSlwXKQ9PERHu8ESbul7CPp\nz5KmF/1cjFZF8rhc0jvKOrikq6Hx50LSEkmDzuyearAcJP2DpNWSbpB0taT06cnHoUoUY+A44L/z\n//c7CHhdzfIcYNTFGNgReLIYR8RdEdGUN2SjIuIr+XCkozWiYgycAIyoGI9yn62mZsLcMu0JlFaM\nIyLlc1G4QV6TPwFHRMTzgTNw63lwIxnJqBk3YAqwDtgHuDlfty3wF+A+YBXwMeCv+XargJcBM4Af\nANfmt5fk+y4ELgKWAbcDH8zXf4dsUtVVwCKyD9Ca/G8TgW8ANwArgVfk608gm+nk58CtwHmD5H8o\n8MP8/pvyY2ybx7w9X/+cPMYKskld96vJtasmzuqa/NYMlwPwaaA33/5iYHvgp8D1wBrg2AF5HgNs\nAm7O95lENuP3yvxxXwRs18A+fwY+BVyX79f/WLbPY/w+j/mmQZ6rXYFf5bHWAC/L1x+Xx1oDnFuz\n/aYBuSzJ7y8hGznwd8BnycZN+a/8sV8HPCff7kSy98Zq4FOD5NOex1qTH/8jdV6vJcAXgKvJ3lvH\n5OuvIRtQaxXwkTzuoppj/998uzlk78vvAzflr5tqXv+r88fwe2DqUHEGeRyb6sUfsP2SmtxPy+Ov\nISuSyh//dTXb792/DMwGfpk/N0uBXfP1y4DPAcuB+cN83qcB68quO1W8lZ8A/APw9fz+1cDs/P4J\nwJdqtltIXrjy5UuAl+b39wD+ULPd1cB2ZNcV3g9sQ03xzbd7chmYD1yU39+P7B+CiXkOtwMd+fId\nwO4D8p/AU0W3O39jvwQ4Avj3fP0VwN75/RcCVw58TPmH4fD8/qd5ejEeNAeeXqzeAlxYs9wxyHO9\nDDgkvz8RuBPYJ1/+JvDh4fbJl/8MfCC//17ga/n9s4Hj8/s7ArcA2w+INR/4RH6/nazg7JY/3zPy\n5/JK4OhBHt/AYvwToD1f/h3ZNGD9j2sy8GqeKi5t+fYvH5DPbOAXNcs71nm9lgDfy+MdANyWr58D\n/KQmzjzg1Pz+dmQFaq98uw1AZx7jt8BLyf7xvh04NN9nh/y5GDTOIK9RbTH+H/EH2X4JTxXjnWrW\nfwt4Q37/KuCgmtf2A2Sfo6uBGfn6Y3nqc7MMuKCBz3tX/3vGt6ffqvAV7zjg8/n97+TLK4be/ElH\nAQfkk6QC7CBpSn7/pxHxOPC4pHuBXerEeinwRYCIuEnSHWQtdYArImIDgKS1wLPIihj59lsk/VHS\n/sBhZC21l5MVm1/nOb0Y+F5NrtvVHlzSjsDUiPhtvuoS4PU1mwybQ+4G4DP5/IQ/iYhf13nM+wJ/\niohb8uV/A95H1rqp54f5/1cAf5/ffzXwRkn9Q6tOJP9Hsma/a4GLJG0D/DgiVkk6ElgWEfflj+9i\nsufvx3Vy+F5E9EqaCsyMiB8BRMRjeZxX5zmtzLefQtbC+1VNjNuBZ0v6Itm3issbeL1+HNkYi2sl\nDfW+ejXwgpp+2Y782E8Av4+InjzHVWSNgg3A3RFxbf4Y/lbzGAaL86dhnpfB4v/3MNu/Ih8QbDKw\nE3Aj8P+Ar5FNQvxRsqJ7GNl75nnAL/Lnph24uybWd4c5DvnExu8i+7zZAKUWY0k7AUcCz5cUZC9u\nSDqxgd3bgBf1f/hqYgI8XrOql7TH2UisXwGvBTaTfV1eQvZYTszzfCgiDmpmDhFxi6SDyfrZz5R0\nRUScnnDMRvKpzUXAWyLi5qF2iohfSXo58HfAEkmfJStEQ+5Sc3/igL89XCdHAedExFeHyefB/GTS\na4B/Ad4GfJjhX6/a12Ko8VlE9u1h6dNWSnMY2Xtz0Dh1NBxf0kSyuSwPiYg7JS3kqef5B8Anyb6p\nrIiI+/MTuTdGxOFDhBzyNZH0ArIC/9qIuL/RB9NKyj6BdwzwrYh4VkTsGRG7k/2r/zJgI9nX2H4D\nly8n++oEZFdf1DnWwP1r/ZqsuwRJ+5C16IYsKkPs/2Hgt3kL7xlkrYg1eSvnT5LemsfXwLPJEfEQ\nsFHSC/NVtaPlDWdz3sok/6A8EhHfJutnPHiQ7Wufg5uBPSX1j1P9TrK+wOH2Gc5S4APK/zWUNGvg\nBpKeBdwTEReSfTAPJusfPULSdEntZN+M+vO4R9L++TRfbx7soBGxEeiRdHR+jO0kTc7z+ef+b0uS\nZkraeUA+04G2iPgBcCpwcCOv1yAGPkdLgffUvDb7qGa2nEHcDOwq6dB8+6n5SbCRxhmp/sK7Pn+e\nnjyhnTdylgJfJjuf0p/nDEmH5/lsI+l/1TuIpD3Ivk29s+abmA1QdjE+DvjRgHU/yNdfRdYNsUrS\nsWRfnd6cL78M+CBwSH7JzFqyls2Q8n+NfyNpjaRFA/58AdAm6Qayr1on5N0cjfodWVdI/1fg1cAN\nkXeSkRX6d0m6nuxr4GCzar8LuDD/ark9w7cY+y0GVudf7Z8P/D7f/5PAmYNsvwT4Sr6NgH8i+zp+\nA9BHdlJsyH0kTRomlzPI+pCz4ocAAAELSURBVBRXS7oxXx5oDnC9pJVkX30/HxF3AyeTvd7Xk7XC\n/jPf/mSyvt6refrX4YHeCXxQ0up822dGxOVk3T2/zR/f9/mf/6jMBJblz8e3eWqY2EZer1qrgV5J\n10v6CNk/NGuB6yStAb7KMC3UiHgifz6+mB/zF2SFckRxRipvBFxIdr5iKVk3Uq2Lyd4Xl9fkeQxw\nbp7nKhq7wuk0sgbKBfn7aHkxj2B8GRNjU7QCSVMiYlN+/2Sys9QfKjkta2F5/39HRCwoO5dWUIUT\neJb5O0mnkL0md5BdRWFWCkk/IrvE7ciyc2kVbhmbmVVA2X3GZmaGi7GZWSW4GJuZVYCLsZlZBbgY\nm5lVwP8HfmUuT2asxRcAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAWMAAAEYCAYAAACJJ5fjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLsZmZiXw323+fCCT5jsSAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["'eetstry'"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"id":"owstslMF-wdN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}